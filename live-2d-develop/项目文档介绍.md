# LLM 客户端代码分析

## js\ai\llm-client.js

一个封装好的LLM的API调用类，包括处理response以及错误处理操作。

**问题：** 虽然chatCompletion函数里面有stream的选项，但是这个类并没有实现流式的输出处理。开什么玩笑！！！？？！！

---

## js\ai\llm-handler.js

一个基于工具调用（function call和mcp）和图片处理的类。

大致流程：输入 + llm处理 + tts，结尾是一些错误处理。

**问题：**

整个代码耦合很高。同时function函数担负了太多的职责。

tool的多工具处理有问题，不应该有向后兼容的逻辑。

工具调用里面第一个messages的消息添加：
```javascript
'role': 'assistant',
'content': null,
'tool_calls': result.tool_calls
```
其中的content是否为null可待讨论。个人感觉应该改为模型的content。

同时这个工具并没有使用流式功能。因为这一行：
```javascript
const result = await llmClient.chatCompletion(messagesForAPI, allTools);
```
是导入的llmclient的类，这个类就是上面的那个llm-client.js代码里面的：
```javascript
chatCompletion(messages, tools = null, stream = false)
```
可以发现result最后并没有填写stream的形参，所以默认就是非流式工具调用。按理来说得流式的工具调用。

---

## js\ai\ContextManager.js

一个管理LLM的上下文窗口的类。

对话长度超出阈值就直接裁剪：`setMaxContextMessages(count)`

同时管理着"对话历史.jsonl"和AI记录室，特别是对话历史.jsonl，有很多的记录逻辑。

**问题：** 依旧是耦合度太高。不清楚这个到底是处理对话记忆.jsonl的上下文管理器还是LLM的上下文管理。

---

未完待续....

