# 解读前端代码
前言。因为代码大部分都是AI写的。我只负责压榨AI，只告诉实现的逻辑思路。具体代码都让AI来搞，也很少去审核代码，所以这里是解读AI写的代码。顺便让我自己也理解代码的整体设计。
找出不合理的地方逐个攻破

## js\ai\llm-client.js

一个封装好的LLM的API调用类，包括处理response以及错误处理操作。

**问题：** 虽然chatCompletion函数里面有stream的选项，但是这个类并没有实现流式的输出处理。开什么玩笑！！！？？！！

---

## js\ai\llm-handler.js

一个基于工具调用（function call和mcp）和图片处理的类。

大致流程：输入 + llm处理 + tts，结尾是一些错误处理。

**问题：**

整个代码耦合很高。同时匿名函数担负了太多的职责。

tool的多工具处理有问题，不应该有向后兼容的逻辑。

工具调用里面第一个messages的消息添加：
```javascript
'role': 'assistant',
'content': null,
'tool_calls': result.tool_calls
```
其中的content是否为null可待讨论。个人感觉应该改为模型的content。

同时这个工具并没有使用流式功能。因为这一行：
```javascript
const result = await llmClient.chatCompletion(messagesForAPI, allTools);
```
是导入的llmclient的类，这个类就是上面的那个llm-client.js代码里面的：
```javascript
chatCompletion(messages, tools = null, stream = false)
```
可以发现result最后并没有填写stream的形参，所以默认就是非流式工具调用。按理来说得流式的工具调用。

---

## js\ai\ContextManager.js

一个管理LLM的上下文窗口的类。

对话长度超出阈值就直接裁剪：`setMaxContextMessages(count)`

管理着"对话历史.jsonl"有很多的记录逻辑。

**问题：** 依旧是耦合度太高。不清楚这个到底是处理对话记忆.jsonl的上下文管理器还是LLM的上下文管理。

---

## js\ai\DiaryManager.js

一个用于记录AI日记的代码。

generateDiary函数内的diaryPrompt变量直接硬编码了肥牛的角色。正确做法应该是动态加载用户的系统提示词进行加载。

**问题：** 处理过于繁琐，可读性太差


## js\ai\GameIntegration.js

这个是用于连接MC我的世界的模块代码

通过连接asr的输入模块，将内容传递到mc游戏ai里



## js\ai\MemoryManager.js

这个是bert的一个记忆模块。用于读取用户的输入，判断是否需要触发记忆写入

通过用户的输入，首先经过bert API进行分类。如果输入内容是关于记录事件、用户个人相关的情况这类内容，bert可能会触发识别功能。
从而判定为需要记录。
主要处理逻辑在这两个函数内：callBertClassifier(text) checkAndSaveMemoryAsync(text)

当触发后，会把用户的内容直接异步处理让AI进行以下提示词的总结操作：

基于以下对话上下文，将用户的最新消息总结为不超过15个字的关键信息：

对话上下文：
${recentContext}

用户最新消息：${userText}

请提取关键信息（限制15字以内）

最终记录到AI记录室中的：核心用户记忆.txt 文件里面。


未完待续....









