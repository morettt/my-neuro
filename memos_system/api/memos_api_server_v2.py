# memos_api_server_v2.py - MemOS FastAPI æœåŠ¡ï¼ˆå®Œæ•´é›†æˆç‰ˆï¼‰
"""
é›†æˆ Qdrant å‘é‡æ•°æ®åº“ã€Neo4j çŸ¥è¯†å›¾è°±çš„å®Œæ•´ç‰ˆ MemOS API

æ–°åŠŸèƒ½ï¼š
- Qdrant å‘é‡æ•°æ®åº“ï¼ˆæ›¿ä»£ JSON å­˜å‚¨ï¼‰
- Neo4j çŸ¥è¯†å›¾è°±ï¼ˆå¯é€‰ï¼‰
- æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + BM25ï¼‰
- å®ä½“å…³ç³»æå–
- å¤šç”¨æˆ·æ”¯æŒ
- MemCube å®¹å™¨
"""

from fastapi import FastAPI, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
from contextlib import asynccontextmanager
import uvicorn
import json
import os
import re
import asyncio
import uuid
import logging
from datetime import datetime

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
embedding_model = None
qdrant_client = None
neo4j_client = None
config = None
llm_config = None
full_config = None
bm25_searcher = None
preference_memory = None  # åå¥½è®°å¿†ç®¡ç†å™¨
tool_memory = None        # å·¥å…·è®°å¿†ç®¡ç†å™¨
document_loader = None    # æ–‡æ¡£åŠ è½½å™¨
scheduler = None          # å¼‚æ­¥ä»»åŠ¡è°ƒåº¦å™¨
image_memory = None       # å›¾åƒè®°å¿†ç®¡ç†å™¨
entity_extractor = None   # å®ä½“æå–å™¨

# è®°å¿†ç±»å‹æƒé‡é…ç½®ï¼ˆæœç´¢æ—¶åŠ æƒï¼‰
MEMORY_TYPE_WEIGHTS = {
    'preference': 1.5,    # åå¥½è®°å¿†æƒé‡æœ€é«˜
    'fact': 1.3,          # äº‹å®è®°å¿†
    'semantic': 1.2,      # è¯­ä¹‰è®°å¿†
    'episodic': 1.0,      # æƒ…æ™¯è®°å¿†ï¼ˆåŸºå‡†ï¼‰
    'procedural': 1.1,    # ç¨‹åºè®°å¿†
    'event': 1.0,         # äº‹ä»¶è®°å¿†
    'tool': 0.9,          # å·¥å…·è®°å¿†
    'general': 1.0,       # é€šç”¨è®°å¿†
}

# å…¼å®¹æ—§ç‰ˆï¼šå†…å­˜å¤‡ä»½å­˜å‚¨
memory_store_backup = []
USER_ID = "feiniu_default"


# ==================== ç”Ÿå‘½å‘¨æœŸç®¡ç† ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    await startup_event()
    yield
    # å…³é—­æ—¶æ¸…ç†
    await shutdown_event()


app = FastAPI(
    title="MemOS API for è‚¥ç‰›AI",
    version="2.0.0",
    description="é›†æˆ Qdrant + Neo4j çš„å®Œæ•´ç‰ˆè®°å¿†ç³»ç»Ÿ",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== è¯·æ±‚æ¨¡å‹ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰ ====================

class AddMemoryRequest(BaseModel):
    messages: List[Dict[str, str]]
    user_id: Optional[str] = USER_ID


class RawMemoryMessage(BaseModel):
    content: str
    role: Optional[str] = "user"
    importance: Optional[float] = 0.8
    memory_type: Optional[str] = "general"  # æ”¯æŒæŒ‡å®šè®°å¿†ç±»å‹
    tags: Optional[List[str]] = None  # æ ‡ç­¾


class AddRawMemoryRequest(BaseModel):
    messages: List[RawMemoryMessage]
    user_id: Optional[str] = USER_ID
    extract_entities: Optional[bool] = False  # æ˜¯å¦æå–å®ä½“åˆ°å›¾è°±


class SearchMemoryRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    user_id: Optional[str] = USER_ID
    similarity_threshold: Optional[float] = 0.5
    use_graph: Optional[bool] = None   # æ˜¯å¦ä½¿ç”¨å›¾å¢å¼ºï¼ˆNoneè¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰
    use_bm25: Optional[bool] = None    # æ˜¯å¦ä½¿ç”¨BM25æ··åˆæœç´¢ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰
    tags: Optional[List[str]] = None   # æ ‡ç­¾è¿‡æ»¤
    memory_types: Optional[List[str]] = None  # è®°å¿†ç±»å‹è¿‡æ»¤


class MigrateRequest(BaseModel):
    file_path: str


# ==================== åˆå§‹åŒ– ====================

async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
    global embedding_model, qdrant_client, neo4j_client, config
    global llm_config, full_config, bm25_searcher, memory_store_backup
    
    print("=" * 60)
    print("  [å¯åŠ¨] MemOS æœåŠ¡ï¼ˆå®Œæ•´é›†æˆç‰ˆ v2.0ï¼‰")
    print("=" * 60)
    
    try:
        # 1. åŠ è½½é…ç½®
        config_path = os.path.join(os.path.dirname(__file__), "..", "config", "memos_config.json")
        print(f"[é…ç½®] é…ç½®æ–‡ä»¶: {config_path}")
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                full_config = config
                llm_config = config.get('llm', {}).get('config', {})
                
                if llm_config and all(llm_config.get(k) for k in ['model', 'api_key', 'base_url']):
                    print(f"[OK] LLM é…ç½®: {llm_config.get('model')}")
                else:
                    print("[è­¦å‘Š] LLM é…ç½®ä¸å®Œæ•´")
        else:
            print(f"[è­¦å‘Š] é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            config = {}
        
        # 2. åŠ è½½ Embedding æ¨¡å‹
        print("[åŠ è½½] Embedding æ¨¡å‹...")
        from sentence_transformers import SentenceTransformer
        import torch
        
        model_path = config.get('embedding', {}).get('model_path', '../full-hub/rag-hub')
        if not os.path.isabs(model_path):
            model_path = os.path.join(os.path.dirname(__file__), "..", model_path)
        model_path = os.path.normpath(model_path)
        
        embedding_model = SentenceTransformer(model_path)
        if torch.cuda.is_available():
            embedding_model = embedding_model.to('cuda')
            print("[OK] Embedding æ¨¡å‹å·²åŠ è½½ (GPU)")
        else:
            print("[OK] Embedding æ¨¡å‹å·²åŠ è½½ (CPU)")
        
        # 3. åˆå§‹åŒ– Qdrant
        print("[åˆå§‹åŒ–] Qdrant å‘é‡æ•°æ®åº“...")
        try:
            import sys
            sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
            from storage.qdrant_client import MemosQdrantClient
            
            qdrant_path = config.get('storage', {}).get('vector', {}).get('path', './data/qdrant')
            if not os.path.isabs(qdrant_path):
                qdrant_path = os.path.join(os.path.dirname(__file__), "..", qdrant_path)
            qdrant_path = os.path.normpath(qdrant_path)
            
            vector_size = config.get('embedding', {}).get('vector_size', 768)
            collection_name = config.get('storage', {}).get('vector', {}).get('collection_name', 'memories')
            
            qdrant_client = MemosQdrantClient(
                path=qdrant_path,
                collection_name=collection_name,
                vector_size=vector_size
            )
            
            if qdrant_client.is_available():
                info = qdrant_client.get_collection_info()
                print(f"[OK] Qdrant å·²å°±ç»ª: {info.get('points_count', 0)} æ¡è®°å¿†")
            else:
                print("[è­¦å‘Š] Qdrant åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜å­˜å‚¨")
                qdrant_client = None
        except ImportError as e:
            print(f"[è­¦å‘Š] Qdrant æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            print("   è¯·è¿è¡Œ: pip install qdrant-client")
            qdrant_client = None
        except Exception as e:
            print(f"[è­¦å‘Š] Qdrant åˆå§‹åŒ–å¤±è´¥: {e}")
            qdrant_client = None
        
        # 4. åˆå§‹åŒ–å›¾æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        graph_enabled = config.get('storage', {}).get('graph', {}).get('enabled', False)
        if graph_enabled:
            graph_type = config.get('storage', {}).get('graph', {}).get('type', 'networkx')
            print(f"[åˆå§‹åŒ–] å›¾æ•°æ®åº“ ({graph_type})...")
            try:
                graph_config = config.get('storage', {}).get('graph', {})
                
                if graph_type == 'networkx':
                    # ä½¿ç”¨è½»é‡çº§ NetworkX å›¾å­˜å‚¨
                    from storage.networkx_graph import NetworkXGraphClient
                    graph_path = graph_config.get('path', './data/graph_store.json')
                    if not os.path.isabs(graph_path):
                        graph_path = os.path.join(os.path.dirname(__file__), "..", graph_path)
                    neo4j_client = NetworkXGraphClient(data_path=graph_path)
                else:
                    # ä½¿ç”¨ Neo4j
                    from storage.neo4j_client import MemosNeo4jClient
                    neo4j_client = MemosNeo4jClient(
                        uri=graph_config.get('uri', 'bolt://localhost:7687'),
                        user=graph_config.get('user', 'neo4j'),
                        password=graph_config.get('password', 'password')
                    )
                
                if neo4j_client.is_available():
                    stats = neo4j_client.get_stats()
                    print(f"[OK] å›¾æ•°æ®åº“å·²å°±ç»ª: {stats.get('entity_count', 0)} å®ä½“, {stats.get('relation_count', 0)} å…³ç³»")
                else:
                    print("[è­¦å‘Š] å›¾æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
                    neo4j_client = None
            except Exception as e:
                print(f"[è­¦å‘Š] å›¾æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
                neo4j_client = None
        else:
            print("[ä¿¡æ¯] å›¾æ•°æ®åº“æœªå¯ç”¨")
        
        # 5. åŠ è½½å¤‡ä»½æ•°æ®ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
        legacy_json = config.get('storage', {}).get('legacy_json', {})
        if legacy_json.get('enabled', True):
            json_path = legacy_json.get('path', './data/memory_store.json')
            if not os.path.isabs(json_path):
                json_path = os.path.join(os.path.dirname(__file__), "..", json_path)
            
            if os.path.exists(json_path):
                try:
                    with open(json_path, 'r', encoding='utf-8') as f:
                        memory_store_backup = json.load(f)
                    print(f"[OK] åŠ è½½ JSON å¤‡ä»½: {len(memory_store_backup)} æ¡è®°å¿†")
                    
                    # å¦‚æœ Qdrant ä¸ºç©ºï¼Œå°è¯•è¿ç§»
                    if qdrant_client and qdrant_client.is_available():
                        qdrant_count = qdrant_client.count_memories()
                        if qdrant_count == 0 and len(memory_store_backup) > 0:
                            print("[è¿ç§»] æ£€æµ‹åˆ°éœ€è¦è¿ç§»æ•°æ®åˆ° Qdrant...")
                            await migrate_json_to_qdrant()
                except Exception as e:
                    print(f"[è­¦å‘Š] åŠ è½½ JSON å¤‡ä»½å¤±è´¥: {e}")
        
        # 6. åˆå§‹åŒ– BM25 ç´¢å¼•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.get('search', {}).get('enable_bm25', False):
            print("[åˆå§‹åŒ–] BM25 ç´¢å¼•...")
            try:
                from utils.search_utils import BM25Searcher
                bm25_searcher = BM25Searcher()
                await rebuild_bm25_index()
                print("[OK] BM25 ç´¢å¼•å·²å°±ç»ª")
            except Exception as e:
                print(f"[è­¦å‘Š] BM25 åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # 7. åˆå§‹åŒ–åå¥½è®°å¿†ç®¡ç†å™¨
        global preference_memory
        print("[åˆå§‹åŒ–] åå¥½è®°å¿†ç®¡ç†å™¨...")
        try:
            from memories.preference_memory import PreferenceMemory
            preference_memory = PreferenceMemory(
                user_id=USER_ID,
                vector_storage=qdrant_client,
                graph_storage=neo4j_client,
                embedder=embedding_model
            )
            await preference_memory.load()
            pref_summary = await preference_memory.get_summary()
            print(f"[OK] åå¥½è®°å¿†å·²å°±ç»ª: {pref_summary.get('total_count', 0)} ä¸ªåå¥½")
        except Exception as e:
            print(f"[è­¦å‘Š] åå¥½è®°å¿†åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            preference_memory = None
        
        # 8. åˆå§‹åŒ–å·¥å…·è®°å¿†ç®¡ç†å™¨
        global tool_memory
        print("[åˆå§‹åŒ–] å·¥å…·è®°å¿†ç®¡ç†å™¨...")
        try:
            from memories.tool_memory import ToolMemory
            tool_memory = ToolMemory(
                user_id=USER_ID,
                vector_storage=qdrant_client
            )
            await tool_memory.load()
            tool_stats = await tool_memory.get_stats()
            print(f"[OK] å·¥å…·è®°å¿†å·²å°±ç»ª: {tool_stats.get('total_usage', 0)} æ¡ä½¿ç”¨è®°å½•")
        except Exception as e:
            print(f"[è­¦å‘Š] å·¥å…·è®°å¿†åˆå§‹åŒ–å¤±è´¥: {e}")
            tool_memory = None
        
        # 9. åˆå§‹åŒ–æ–‡æ¡£åŠ è½½å™¨
        global document_loader
        print("[åˆå§‹åŒ–] æ–‡æ¡£åŠ è½½å™¨...")
        try:
            from utils.document_loader import DocumentLoader
            document_loader = DocumentLoader(
                chunk_size=config.get('kb', {}).get('chunk_size', 500),
                chunk_overlap=config.get('kb', {}).get('chunk_overlap', 50)
            )
            print("[OK] æ–‡æ¡£åŠ è½½å™¨å·²å°±ç»ª")
        except Exception as e:
            print(f"[è­¦å‘Š] æ–‡æ¡£åŠ è½½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            document_loader = None
        
        # 10. åˆå§‹åŒ–å¼‚æ­¥ä»»åŠ¡è°ƒåº¦å™¨
        global scheduler
        scheduler_config = config.get('scheduler', {}) if config else {}
        if scheduler_config.get('enabled', False):
            print("[åˆå§‹åŒ–] å¼‚æ­¥ä»»åŠ¡è°ƒåº¦å™¨...")
            try:
                from core.scheduler import MemScheduler
                scheduler = MemScheduler(
                    use_redis=scheduler_config.get('use_redis', False),
                    redis_url=scheduler_config.get('redis_url', 'redis://localhost:6379'),
                    max_workers=scheduler_config.get('max_workers', 4),
                    quota_per_user=scheduler_config.get('quota_per_user', 100)
                )
                await scheduler.start()
                
                # æ³¨å†Œä»»åŠ¡å¤„ç†å™¨
                scheduler.register_handler('add_memory', _handle_add_memory_task)
                scheduler.register_handler('process_image', _handle_process_image_task)
                scheduler.register_handler('extract_entities', _handle_extract_entities_task)
                
                print(f"[OK] è°ƒåº¦å™¨å·²å°±ç»ª: {scheduler_config.get('max_workers', 4)} ä¸ªå·¥ä½œåç¨‹")
            except Exception as e:
                print(f"[è­¦å‘Š] è°ƒåº¦å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                scheduler = None
        else:
            print("[ä¿¡æ¯] å¼‚æ­¥è°ƒåº¦å™¨æœªå¯ç”¨ï¼ˆå¯åœ¨é…ç½®ä¸­å¯ç”¨ï¼‰")
        
        # 11. åˆå§‹åŒ–å›¾åƒè®°å¿†ç®¡ç†å™¨
        global image_memory
        image_config = config.get('image', {}) if config else {}
        if image_config.get('enabled', True):
            print("[åˆå§‹åŒ–] å›¾åƒè®°å¿†ç®¡ç†å™¨...")
            try:
                from memories.image_memory import ImageMemory
                image_storage_path = image_config.get('storage_path', './data/images')
                if not os.path.isabs(image_storage_path):
                    image_storage_path = os.path.join(os.path.dirname(__file__), "..", image_storage_path)
                
                image_memory = ImageMemory(
                    storage_path=image_storage_path,
                    vector_storage=qdrant_client,
                    embedder=embedding_model,
                    llm_config=llm_config,
                    use_clip=image_config.get('use_clip', False),
                    max_image_size=image_config.get('max_size_mb', 5) * 1024 * 1024
                )
                await image_memory.load_metadata()
                img_stats = image_memory.get_stats()
                print(f"[OK] å›¾åƒè®°å¿†å·²å°±ç»ª: {img_stats.get('total_images', 0)} å¼ å›¾åƒ")
            except Exception as e:
                print(f"[è­¦å‘Š] å›¾åƒè®°å¿†åˆå§‹åŒ–å¤±è´¥: {e}")
                image_memory = None
        
        # 12. åˆå§‹åŒ–å®ä½“æå–å™¨
        global entity_extractor
        entity_config = config.get('entity_extraction', {}) if config else {}
        if entity_config.get('enabled', False) and llm_config:
            print("[åˆå§‹åŒ–] å®ä½“æå–å™¨...")
            try:
                from utils.entity_extractor import EntityExtractor
                entity_extractor = EntityExtractor(
                    llm_config=llm_config,
                    fallback_config=full_config.get('llm_fallback', {}).get('config') if full_config else None
                )
                print("[OK] å®ä½“æå–å™¨å·²å°±ç»ª")
            except Exception as e:
                print(f"[è­¦å‘Š] å®ä½“æå–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                entity_extractor = None
        else:
            print("[ä¿¡æ¯] å®ä½“æå–å™¨æœªå¯ç”¨ï¼ˆå¯åœ¨é…ç½®ä¸­å¯ç”¨ï¼‰")
        
        print("=" * 60)
        print("  [OK] MemOS æœåŠ¡å¯åŠ¨æˆåŠŸ!")
        print("=" * 60)
        print(f"  Qdrant: {'å·²å¯ç”¨' if qdrant_client else 'æœªå¯ç”¨'}")
        print(f"  Graph: {'å·²å¯ç”¨' if neo4j_client else 'æœªå¯ç”¨'}")
        print(f"  Scheduler: {'å·²å¯ç”¨' if scheduler else 'æœªå¯ç”¨'}")
        print(f"  Image: {'å·²å¯ç”¨' if image_memory else 'æœªå¯ç”¨'}")
        print(f"  Entity: {'å·²å¯ç”¨' if entity_extractor else 'æœªå¯ç”¨'}")
        print(f"  LLM: {llm_config.get('model', 'æœªé…ç½®') if llm_config else 'æœªé…ç½®'}")
        print("=" * 60)
        
    except Exception as e:
        print(f"[é”™è¯¯] åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# ==================== è°ƒåº¦å™¨ä»»åŠ¡å¤„ç†å™¨ ====================

async def _handle_add_memory_task(task):
    """å¤„ç†æ·»åŠ è®°å¿†ä»»åŠ¡"""
    from core.scheduler import Task
    payload = task.payload
    
    content = payload.get('content', '')
    user_id = payload.get('user_id', USER_ID)
    importance = payload.get('importance', 0.5)
    memory_type = payload.get('memory_type', 'general')
    
    if not content:
        return {'status': 'error', 'message': 'å†…å®¹ä¸ºç©º'}
    
    vector = encode_text(content)
    memory_id = str(uuid.uuid4())
    
    qdrant_payload = {
        'content': content,
        'user_id': user_id,
        'importance': importance,
        'memory_type': memory_type,
        'created_at': datetime.now().isoformat(),
        'async_processed': True
    }
    
    if qdrant_client and qdrant_client.is_available():
        qdrant_client.add_memory(memory_id, vector, qdrant_payload)
        return {'status': 'success', 'memory_id': memory_id}
    
    return {'status': 'error', 'message': 'å­˜å‚¨ä¸å¯ç”¨'}


async def _handle_process_image_task(task):
    """å¤„ç†å›¾åƒä»»åŠ¡"""
    payload = task.payload
    
    if not image_memory:
        return {'status': 'error', 'message': 'å›¾åƒè®°å¿†æœªå¯ç”¨'}
    
    image_data = payload.get('image_data')
    if not image_data:
        return {'status': 'error', 'message': 'å›¾åƒæ•°æ®ä¸ºç©º'}
    
    result = await image_memory.save_image_from_base64(
        image_data,
        original_name=payload.get('filename', 'image.jpg'),
        image_type=payload.get('image_type', 'other'),
        description=payload.get('description'),
        tags=payload.get('tags', []),
        user_id=payload.get('user_id', USER_ID)
    )
    
    if result:
        return {'status': 'success', 'image_id': result.id}
    return {'status': 'error', 'message': 'å¤„ç†å¤±è´¥'}


async def _handle_extract_entities_task(task):
    """å¤„ç†å®ä½“æå–ä»»åŠ¡"""
    payload = task.payload
    content = payload.get('content', '')
    
    if not content:
        return {'status': 'error', 'message': 'å†…å®¹ä¸ºç©º'}
    
    # TODO: å®ç°å®ä½“æå–é€»è¾‘
    return {'status': 'success', 'entities': []}


async def shutdown_event():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    global qdrant_client, neo4j_client, scheduler
    
    print("[å…³é—­] æ­£åœ¨å…³é—­ MemOS æœåŠ¡...")
    
    # å…³é—­è°ƒåº¦å™¨
    if scheduler:
        try:
            await scheduler.stop()
            print("[OK] è°ƒåº¦å™¨å·²åœæ­¢")
        except:
            pass
    
    if qdrant_client:
        try:
            qdrant_client.close()
        except:
            pass
    
    if neo4j_client:
        try:
            neo4j_client.close()
        except:
            pass
    
    print("[OK] MemOS æœåŠ¡å·²å…³é—­")


async def migrate_json_to_qdrant():
    """å°† JSON æ•°æ®è¿ç§»åˆ° Qdrant"""
    global memory_store_backup, qdrant_client
    
    if not qdrant_client or not memory_store_backup:
        return
    
    print(f"[è¿ç§»] å¼€å§‹è¿ç§» {len(memory_store_backup)} æ¡è®°å¿†åˆ° Qdrant...")
    
    batch = []
    migrated = 0
    
    for i, mem in enumerate(memory_store_backup):
        content = mem.get('content', '')
        if not content or len(content) < 5:
            continue
        
        # è·å–æˆ–ç”Ÿæˆå‘é‡
        if 'embedding' in mem and mem['embedding']:
            vector = mem['embedding']
        else:
            vector = embedding_model.encode([content])[0].tolist()
        
        # æ„å»º payload
        payload = {
            'content': content,
            'user_id': mem.get('user_id', USER_ID),
            'importance': mem.get('importance', 0.5),
            'memory_type': mem.get('memory_type', 'general'),
            'tags': mem.get('tags', []),
            'created_at': mem.get('created_at') or mem.get('timestamp', datetime.now().isoformat()),
            'updated_at': mem.get('updated_at'),
            'merge_count': mem.get('merge_count', 0),
            'source': 'migrated_from_json'
        }
        
        memory_id = mem.get('id') or f"migrated_{i}_{uuid.uuid4().hex[:8]}"
        
        batch.append({
            'id': memory_id,
            'vector': vector,
            'payload': payload
        })
        
        if len(batch) >= 50:
            count = qdrant_client.add_memories_batch(batch)
            migrated += count
            batch = []
            print(f"  è¿›åº¦: {migrated}/{len(memory_store_backup)}")
    
    if batch:
        count = qdrant_client.add_memories_batch(batch)
        migrated += count
    
    print(f"[OK] è¿ç§»å®Œæˆ: {migrated} æ¡è®°å¿†")


async def rebuild_bm25_index():
    """é‡å»º BM25 ç´¢å¼•"""
    global bm25_searcher, qdrant_client
    
    if not bm25_searcher:
        return
    
    # ä» Qdrant è·å–æ‰€æœ‰æ–‡æ¡£
    if qdrant_client and qdrant_client.is_available():
        documents = qdrant_client.get_all_memories(limit=10000)
        bm25_searcher.build_index(documents)


def update_bm25_index(memory_id: str, content: str):
    """å¢é‡æ›´æ–° BM25 ç´¢å¼•ï¼ˆæ·»åŠ å•æ¡è®°å¿†ï¼‰"""
    global bm25_searcher
    
    if bm25_searcher and hasattr(bm25_searcher, 'add_document'):
        try:
            bm25_searcher.add_document(memory_id, content)
        except Exception as e:
            print(f"[è­¦å‘Š] BM25 ç´¢å¼•æ›´æ–°å¤±è´¥: {e}")


# ==================== å·¥å…·å‡½æ•° ====================

def encode_text(text: str) -> List[float]:
    """æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
    if embedding_model:
        return embedding_model.encode([text])[0].tolist()
    return []


def get_storage():
    """è·å–å­˜å‚¨å®¢æˆ·ç«¯"""
    return qdrant_client


async def process_conversation_batch(conversation: str) -> Dict[str, Any]:
    """ä½¿ç”¨ LLM ä»å¯¹è¯ä¸­æå–è®°å¿†"""
    global llm_config, full_config
    
    if not llm_config:
        return {"memories": []}
    
    # æ„å»ºæ¨¡å‹åˆ—è¡¨
    models_to_try = []
    
    api_key = llm_config.get('api_key', '')
    model = llm_config.get('model', '')
    base_url = llm_config.get('base_url', '')
    
    if api_key and model and base_url:
        models_to_try.append({
            'name': 'ä¸»æ¨¡å‹',
            'api_key': api_key,
            'model': model,
            'base_url': base_url
        })
    
    # å¤‡ç”¨æ¨¡å‹
    fallback_config = full_config.get('llm_fallback', {}) if full_config else {}
    if fallback_config.get('enabled', False):
        fb_cfg = fallback_config.get('config', {})
        if all(fb_cfg.get(k) for k in ['api_key', 'model', 'base_url']):
            models_to_try.append({
                'name': 'å¤‡ç”¨æ¨¡å‹',
                **fb_cfg
            })
    
    if not models_to_try:
        return {"memories": []}
    
    import aiohttp
    
    prompt = f"""ä½ æ˜¯è®°å¿†æå–ä¸“å®¶ã€‚ä»ä»¥ä¸‹å¤šè½®å¯¹è¯ä¸­æå–å…³é”®äº‹å®ï¼Œå¹¶æŒ‰ç±»å‹ä¸¥æ ¼åˆ†ç±»ã€‚

èº«ä»½è¯´æ˜ï¼š
- "ä¸»äºº"æ˜¯ä½¿ç”¨AIçš„çœŸäººç”¨æˆ·
- "è‚¥ç‰›"æ˜¯AIåŠ©æ‰‹

æå–è§„åˆ™ï¼š
1. ç”¨è‡ªç„¶çš„ä¸­æ–‡æè¿°è¦ç‚¹ï¼Œæ¯æ¡è®°å¿†15-80å­—
2. å¿½ç•¥æ— æ„ä¹‰çš„é—²èŠ
3. åˆ¤æ–­é‡è¦åº¦ï¼ˆ0.1-1.0ï¼‰
4. **ä¸¥æ ¼æŒ‰ä»¥ä¸‹ç±»å‹åˆ†ç±»è®°å¿†ï¼ˆmemory_typeï¼‰**ï¼š
   - preference: ç”¨æˆ·è¡¨è¾¾çš„å–œå¥½/åå¥½/åŒæ¶ï¼ˆå…³é”®è¯ï¼š"å–œæ¬¢"ã€"ä¸å–œæ¬¢"ã€"è®¨åŒ"ã€"æœ€çˆ±"ã€"åå¥½"ã€"æƒ³è¦"ï¼‰
   - fact: ç”¨æˆ·çš„å®¢è§‚ä¸ªäººä¿¡æ¯ï¼ˆå§“åã€ç”Ÿæ—¥ã€å¹´é¾„ã€èŒä¸šã€åœ°å€ã€èº«ä½“æ•°æ®ã€è”ç³»æ–¹å¼ç­‰ï¼‰
   - episodic: å…·ä½“çš„äº‹ä»¶æˆ–ç»å†ï¼ˆå…³é”®è¯ï¼š"ä»Šå¤©"ã€"æ˜¨å¤©"ã€"åˆšæ‰"ã€"ä¸Šæ¬¡"ã€æœ‰æ˜ç¡®æ—¶é—´/åœ°ç‚¹çš„äº‹ä»¶ï¼‰
   - semantic: ç”¨æˆ·äº†è§£/å­¦ä¹ çš„çŸ¥è¯†æ¦‚å¿µï¼ˆå…³é”®è¯ï¼š"çŸ¥é“"ã€"äº†è§£"ã€"å­¦åˆ°"ã€æŠ€æœ¯çŸ¥è¯†ã€æ¦‚å¿µç†è§£ï¼‰
   - procedural: ç”¨æˆ·çš„æŠ€èƒ½/ä¹ æƒ¯/æ—¥å¸¸è§„å¾‹ï¼ˆå…³é”®è¯ï¼š"ä¼šåš"ã€"ä¹ æƒ¯"ã€"æ¯å¤©éƒ½"ã€"æ€»æ˜¯"ã€"ç»å¸¸"ï¼‰
   - general: æ— æ³•å½’å…¥ä»¥ä¸Šä»»ä½•ç±»åˆ«çš„è®°å¿†ï¼ˆä»…å½“å®Œå…¨ä¸ç¬¦åˆä»¥ä¸Šä»»ä½•ç±»å‹æ—¶ä½¿ç”¨ï¼‰
5. æå–ç›¸å…³æ ‡ç­¾ï¼ˆtagsï¼‰ï¼Œ1-3ä¸ªå…³é”®è¯

**åˆ†ç±»ä¼˜å…ˆçº§ï¼ˆå¦‚æœå¯å½’å…¥å¤šä¸ªç±»å‹ï¼Œé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„ï¼‰**ï¼š
preference > fact > episodic > procedural > semantic > general

**åˆ†ç±»ç¤ºä¾‹**ï¼š
- "ä¸»äººå–œæ¬¢åƒè¾£çš„é£Ÿç‰©" â†’ preferenceï¼ˆè¡¨è¾¾å–œå¥½ï¼‰
- "ä¸»äººçš„ç”Ÿæ—¥æ˜¯5æœˆ20æ—¥" â†’ factï¼ˆä¸ªäººä¿¡æ¯ï¼‰
- "ä¸»äººä»Šå¤©å»äº†å¥èº«æˆ¿é”»ç‚¼" â†’ episodicï¼ˆå…·ä½“äº‹ä»¶ï¼‰
- "ä¸»äººæ¯å¤©æ—©ä¸Šéƒ½ä¼šå–å’–å•¡" â†’ proceduralï¼ˆæ—¥å¸¸ä¹ æƒ¯ï¼‰
- "ä¸»äººäº†è§£Pythonç¼–ç¨‹" â†’ semanticï¼ˆçŸ¥è¯†æŠ€èƒ½ï¼‰

å¯¹è¯å†…å®¹ï¼š
{conversation}

è¯·è¿”å› JSONï¼š
{{"memories": [
  {{"content": "ä¸»äººå–œæ¬¢åƒè¾£çš„é£Ÿç‰©", "importance": 0.9, "memory_type": "preference", "tags": ["é£Ÿç‰©", "å£å‘³"]}},
  {{"content": "ä¸»äººä»Šå¤©å»äº†å¥èº«æˆ¿é”»ç‚¼", "importance": 0.6, "memory_type": "episodic", "tags": ["å¥èº«", "è¿åŠ¨"]}},
  {{"content": "ä¸»äººçš„ç”Ÿæ—¥æ˜¯5æœˆ20æ—¥", "importance": 0.95, "memory_type": "fact", "tags": ["ç”Ÿæ—¥", "ä¸ªäººä¿¡æ¯"]}}
]}}
"""
    
    timeouts = [60, 120]
    
    for model_info in models_to_try:
        for attempt, timeout_seconds in enumerate(timeouts, 1):
            try:
                async with aiohttp.ClientSession() as session:
                    headers = {
                        "Authorization": f"Bearer {model_info['api_key']}",
                        "Content-Type": "application/json"
                    }
                    
                    payload = {
                        "model": model_info['model'],
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": 2000,
                        "temperature": 0.3
                    }
                    
                    async with session.post(
                        f"{model_info['base_url']}/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=timeout_seconds)
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            response_text = result['choices'][0]['message']['content'].strip()
                            
                            try:
                                parsed = json.loads(response_text)
                            except:
                                json_match = re.search(r'\{[\s\S]*"memories"[\s\S]*\}', response_text)
                                if json_match:
                                    parsed = json.loads(json_match.group())
                                else:
                                    continue
                            
                            memories = parsed.get('memories', [])
                            valid_memories = []
                            
                            # æœ‰æ•ˆçš„è®°å¿†ç±»å‹åˆ—è¡¨
                            valid_types = ['preference', 'fact', 'episodic', 'semantic', 'procedural', 'general']
                            
                            for mem in memories:
                                if isinstance(mem, dict) and mem.get('content'):
                                    content = str(mem['content']).strip()
                                    try:
                                        importance = float(mem.get('importance', 0.5))
                                    except:
                                        importance = 0.5
                                    importance = max(0.1, min(1.0, importance))
                                    
                                    # ğŸ”¥ æå– memory_type å¹¶éªŒè¯
                                    memory_type = mem.get('memory_type', 'general')
                                    if memory_type not in valid_types:
                                        memory_type = 'general'
                                    
                                    # ğŸ”¥ æå– tags å¹¶éªŒè¯
                                    tags = mem.get('tags', [])
                                    if not isinstance(tags, list):
                                        tags = []
                                    
                                    if len(content) >= 5:
                                        valid_memories.append({
                                            "content": content,
                                            "importance": importance,
                                            "memory_type": memory_type,  # ğŸ”¥ ä¿ç•™è®°å¿†ç±»å‹
                                            "tags": tags                  # ğŸ”¥ ä¿ç•™æ ‡ç­¾
                                        })
                            
                            return {"memories": valid_memories}
                            
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.warning(f"LLM è°ƒç”¨å¤±è´¥: {e}")
                continue
    
    return {"memories": []}


# ==================== API ç«¯ç‚¹ ====================

@app.get("/")
async def root():
    """æœåŠ¡çŠ¶æ€"""
    return {
        "service": "MemOS API for è‚¥ç‰›AI",
        "version": "2.0.0",
        "status": "running",
        "storage": "qdrant" if qdrant_client else "memory",
        "graph": "neo4j" if neo4j_client else "disabled"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    memory_count = 0
    if qdrant_client and qdrant_client.is_available():
        memory_count = qdrant_client.count_memories()
    
    return {
        "status": "healthy",
        "model_loaded": embedding_model is not None,
        "qdrant_available": qdrant_client is not None and qdrant_client.is_available(),
        "neo4j_available": neo4j_client is not None and neo4j_client.is_available(),
        "memory_count": memory_count
    }


@app.post("/add")
async def add_memory(request: AddMemoryRequest):
    """æ·»åŠ è®°å¿†ï¼ˆLLM åŠ å·¥ç‰ˆï¼‰
    
    è‡ªåŠ¨æ‰§è¡Œï¼š
    1. LLM æå–è®°å¿† â†’ å­˜å…¥ Qdrant
    2. LLM æå–åå¥½ â†’ å­˜å…¥ preference_memory
    3. LLM æå–å®ä½“ â†’ å­˜å…¥çŸ¥è¯†å›¾è°±
    """
    global qdrant_client, preference_memory, entity_extractor, neo4j_client
    
    if not embedding_model:
        raise HTTPException(status_code=500, detail="Embedding æ¨¡å‹æœªåŠ è½½")
    
    try:
        user_id = request.user_id or USER_ID
        added_count = 0
        merged_count = 0
        skipped_count = 0
        preference_count = 0
        entity_count = 0
        
        # åˆå¹¶å¯¹è¯
        conversation_text = []
        for msg in request.messages:
            content = msg.get('content', '')
            role = msg.get('role', 'user')
            if content and len(content.strip()) > 0:
                role_label = "ä¸»äºº" if role == 'user' else "è‚¥ç‰›"
                conversation_text.append(f"ã€{role_label}ã€‘{content}")
        
        if not conversation_text:
            return {"status": "success", "message": "æ— æœ‰æ•ˆå¯¹è¯", "added": 0}
        
        full_conversation = "\n".join(conversation_text)
        
        # ğŸ§  å¯¹è¯æ€»ç»“æ—¥å¿—å¼€å§‹
        print(f"\n{'='*60}")
        print(f"ğŸ§  [è®°å¿†æ€»ç»“] æ­£åœ¨å¤„ç† {len(request.messages)} æ¡å¯¹è¯æ¶ˆæ¯...")
        print(f"{'='*60}")
        
        # ========== 1. LLM æå–è®°å¿† ==========
        processed_result = await process_conversation_batch(full_conversation)
        
        if processed_result.get("memories"):
            # å¤„ç†æ¯æ¡è®°å¿†
            for mem_item in processed_result["memories"]:
                content = mem_item.get("content", "").strip()
                importance = mem_item.get("importance", 0.5)
                # ğŸ”¥ æ–°å¢ï¼šä» LLM æå–è®°å¿†ç±»å‹å’Œæ ‡ç­¾
                memory_type = mem_item.get("memory_type", "general")
                tags = mem_item.get("tags", [])
                
                # éªŒè¯è®°å¿†ç±»å‹
                valid_types = ['preference', 'fact', 'episodic', 'semantic', 'procedural', 'general']
                if memory_type not in valid_types:
                    memory_type = 'general'
                
                # ç¡®ä¿ tags æ˜¯åˆ—è¡¨
                if not isinstance(tags, list):
                    tags = []
                
                if not content or len(content) < 5:
                    continue
                
                if importance < 0.3:
                    skipped_count += 1
                    continue
                
                # ç”Ÿæˆå‘é‡
                vector = encode_text(content)
                
                # å»é‡æ£€æŸ¥
                if qdrant_client and qdrant_client.is_available():
                    similar = qdrant_client.find_similar(
                        vector, threshold=0.95, user_id=user_id
                    )
                    
                    if similar:
                        # æ›´æ–°ç°æœ‰è®°å¿†
                        qdrant_client.update_memory(
                            similar['id'],
                            {
                                'merge_count': similar.get('payload', {}).get('merge_count', 0) + 1,
                                'importance': max(importance, similar.get('importance', 0.5))
                            }
                        )
                        merged_count += 1
                        continue
                
                # æ·»åŠ æ–°è®°å¿†ï¼ˆä½¿ç”¨å®Œæ•´ UUIDï¼‰
                memory_id = str(uuid.uuid4())
                payload = {
                    'content': content,
                    'user_id': user_id,
                    'importance': importance,
                    'memory_type': memory_type,  # ğŸ”¥ ä½¿ç”¨ LLM æå–çš„ç±»å‹
                    'tags': tags,                 # ğŸ”¥ ä½¿ç”¨ LLM æå–çš„æ ‡ç­¾
                    'created_at': datetime.now().isoformat(),
                    'merge_count': 0,
                    'processed': True
                }
                
                if qdrant_client and qdrant_client.is_available():
                    qdrant_client.add_memory(memory_id, vector, payload)
                    # æ›´æ–° BM25 ç´¢å¼•
                    update_bm25_index(memory_id, content)
                
                added_count += 1
                # è¯¦ç»†è®°å¿†æ—¥å¿—ï¼ˆåŒ…å«ç±»å‹å’Œæ ‡ç­¾ï¼‰
                type_label = {'preference': 'åå¥½', 'fact': 'äº‹å®', 'episodic': 'æƒ…æ™¯', 
                             'semantic': 'è¯­ä¹‰', 'procedural': 'ç¨‹åºæ€§', 'general': 'é€šç”¨'}.get(memory_type, memory_type)
                tags_str = f" æ ‡ç­¾:{tags}" if tags else ""
                print(f"   ğŸ“ æ–°å¢è®°å¿†: {content[:60]}{'...' if len(content) > 60 else ''}")
                print(f"      â””â”€ ç±»å‹:{type_label} | é‡è¦åº¦:{importance:.0%}{tags_str}")
                logger.info(f"[OK] æ–°å¢è®°å¿†: {content[:50]}... (ç±»å‹:{memory_type}, é‡è¦åº¦:{importance})")
        
        # ========== 2. è‡ªåŠ¨æå–å®ä½“ ==========
        if entity_extractor and neo4j_client:
            try:
                print(f"\nğŸ•¸ï¸ [å®ä½“æå–] æ­£åœ¨åˆ†æçŸ¥è¯†å›¾è°±å®ä½“...")
                entities, relations = await entity_extractor.extract(full_conversation)
                
                if entities:
                    print(f"   å‘ç° {len(entities)} ä¸ªå®ä½“, {len(relations) if relations else 0} ä¸ªå…³ç³»:")
                    for entity in entities:
                        try:
                            # ExtractedEntity æ˜¯ Pydantic æ¨¡å‹ï¼Œç›´æ¥è®¿é—®å±æ€§
                            entity_name = entity.name if hasattr(entity, 'name') else str(entity)
                            entity_type = entity.entity_type.value if hasattr(entity, 'entity_type') else 'unknown'
                            
                            # æ£€æŸ¥å®ä½“æ˜¯å¦å·²å­˜åœ¨
                            existing = neo4j_client.find_entity_by_name(entity_name, user_id)
                            if not existing:
                                # ç”Ÿæˆå®ä½“ IDï¼ˆuuid å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼‰
                                new_entity_id = str(uuid.uuid4())
                                
                                success = neo4j_client.create_entity(
                                    entity_id=new_entity_id,
                                    name=entity_name,
                                    entity_type=entity_type,
                                    user_id=user_id,
                                    properties={'description': entity.description} if hasattr(entity, 'description') and entity.description else {}
                                )
                                if success:
                                    entity_count += 1
                                    # å®ä½“è¯¦ç»†æ—¥å¿—
                                    print(f"   ğŸ”¹ å®ä½“: {entity_name} [{entity_type}]")
                        except Exception as ee:
                            logger.warning(f"ä¿å­˜å®ä½“å¤±è´¥: {ee}")
                    
                    if entity_count > 0:
                        print(f"   âœ… æˆåŠŸä¿å­˜ {entity_count} ä¸ªå®ä½“")
                        logger.info(f"[OK] è‡ªåŠ¨æå–å®ä½“: {entity_count} ä¸ª")
                else:
                    print(f"   â„¹ï¸ æœªå‘ç°æ–°å®ä½“")
                
                # ä¿å­˜å…³ç³»
                if relations:
                    for rel in relations:
                        try:
                            # ExtractedRelation æ˜¯ Pydantic æ¨¡å‹ï¼Œç›´æ¥è®¿é—®å±æ€§
                            source_name = rel.source_name if hasattr(rel, 'source_name') else ''
                            target_name = rel.target_name if hasattr(rel, 'target_name') else ''
                            relation_type = rel.relation_type.value if hasattr(rel, 'relation_type') else 'related_to'
                            
                            source_entity = neo4j_client.find_entity_by_name(source_name, user_id)
                            target_entity = neo4j_client.find_entity_by_name(target_name, user_id)
                            
                            if source_entity and target_entity:
                                neo4j_client.create_relation(
                                    source_id=source_entity['id'],
                                    target_id=target_entity['id'],
                                    relation_type=relation_type,
                                    properties={'description': rel.description} if hasattr(rel, 'description') and rel.description else {}
                                )
                        except Exception as re:
                            logger.warning(f"ä¿å­˜å…³ç³»å¤±è´¥: {re}")
            except Exception as e:
                logger.warning(f"å®ä½“æå–å¤±è´¥: {e}")
        
        # æ„å»ºè¿”å›ç»“æœ
        result_parts = []
        if added_count > 0:
            result_parts.append(f"æ–°å¢è®°å¿† {added_count} æ¡")
        if merged_count > 0:
            result_parts.append(f"åˆå¹¶ {merged_count} æ¡")
        if skipped_count > 0:
            result_parts.append(f"è·³è¿‡ {skipped_count} æ¡")
        if preference_count > 0:
            result_parts.append(f"æå–åå¥½ {preference_count} æ¡")
        if entity_count > 0:
            result_parts.append(f"æå–å®ä½“ {entity_count} ä¸ª")
        
        message = "ã€".join(result_parts) if result_parts else "æ— æœ‰æ•ˆè®°å¿†"
        
        # ğŸ§  æ€»ç»“æ—¥å¿—
        print(f"\n{'='*60}")
        print(f"ğŸ“Š [è®°å¿†æ€»ç»“å®Œæˆ]")
        print(f"   ğŸ“ æ–°å¢è®°å¿†: {added_count} æ¡")
        print(f"   ğŸ”„ åˆå¹¶è®°å¿†: {merged_count} æ¡")
        print(f"   â­ï¸ è·³è¿‡ä½é‡è¦åº¦: {skipped_count} æ¡")
        print(f"   ğŸ’ æå–åå¥½: {preference_count} æ¡")
        print(f"   ğŸ•¸ï¸ æå–å®ä½“: {entity_count} ä¸ª")
        print(f"{'='*60}\n")
        
        return {
            "status": "success",
            "message": message,
            "added": added_count,
            "merged": merged_count,
            "skipped": skipped_count,
            "preferences_extracted": preference_count,
            "entities_extracted": entity_count
        }
        
    except Exception as e:
        logger.error(f"æ·»åŠ è®°å¿†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/add_raw")
async def add_memory_raw(request: AddRawMemoryRequest):
    """ç›´æ¥æ·»åŠ è®°å¿†ï¼ˆä¸ç»è¿‡ LLM åŠ å·¥ï¼‰
    
    æ”¯æŒæŒ‡å®šè®°å¿†ç±»å‹:
    - episodic: æƒ…æ™¯è®°å¿†
    - semantic: è¯­ä¹‰è®°å¿†
    - procedural: ç¨‹åºè®°å¿†
    - preference: åå¥½è®°å¿†
    - fact: äº‹å®è®°å¿†
    - event: äº‹ä»¶è®°å¿†
    - general: é€šç”¨è®°å¿†
    
    æ”¯æŒ extract_entities=true è‡ªåŠ¨æå–å®ä½“åˆ°çŸ¥è¯†å›¾è°±
    """
    if not embedding_model:
        raise HTTPException(status_code=500, detail="Embedding æ¨¡å‹æœªåŠ è½½")
    
    try:
        user_id = request.user_id or USER_ID
        added_count = 0
        type_counts = {}
        extracted_entities = []
        
        for msg in request.messages:
            content = msg.content
            importance = msg.importance if msg.importance is not None else 0.8
            memory_type = msg.memory_type or "general"
            tags = msg.tags or []
            
            # éªŒè¯è®°å¿†ç±»å‹
            if memory_type not in MEMORY_TYPE_WEIGHTS:
                memory_type = "general"
            
            if content and len(content) > 5:
                vector = encode_text(content)
                
                # å»é‡
                if qdrant_client and qdrant_client.is_available():
                    similar = qdrant_client.find_similar(vector, threshold=0.95, user_id=user_id)
                    if similar:
                        continue
                
                memory_id = str(uuid.uuid4())
                entity_ids = []
                
                # å®ä½“æå–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if request.extract_entities and entity_extractor and neo4j_client:
                    try:
                        entities, relations = await entity_extractor.extract(content)
                        
                        entity_name_to_id = {}
                        for entity in entities:
                            ent_id = f"ent_{uuid.uuid4().hex[:12]}"
                            
                            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                            existing = neo4j_client.find_entity_by_name(entity.name, user_id)
                            if existing:
                                ent_id = existing['id']
                                if hasattr(neo4j_client, 'link_entity_to_memory'):
                                    neo4j_client.link_entity_to_memory(ent_id, memory_id)
                            else:
                                neo4j_client.add_entity(
                                    entity_id=ent_id,
                                    entity_type=entity.entity_type.value,
                                    name=entity.name,
                                    properties={
                                        'description': entity.description,
                                        'confidence': entity.confidence,
                                        'source_memory_ids': [memory_id]
                                    },
                                    user_id=user_id
                                )
                            
                            entity_name_to_id[entity.name] = ent_id
                            entity_ids.append(ent_id)
                        
                        # æ·»åŠ å…³ç³»
                        for relation in relations:
                            src_id = entity_name_to_id.get(relation.source_name)
                            tgt_id = entity_name_to_id.get(relation.target_name)
                            if src_id and tgt_id:
                                neo4j_client.add_relation(
                                    source_id=src_id,
                                    target_id=tgt_id,
                                    relation_type=relation.relation_type.value,
                                    properties={
                                        'description': relation.description,
                                        'confidence': relation.confidence,
                                        'source_memory_id': memory_id
                                    }
                                )
                        
                        extracted_entities.extend([{'id': eid, 'name': name} for name, eid in entity_name_to_id.items()])
                        
                    except Exception as e:
                        print(f"[è­¦å‘Š] å®ä½“æå–å¤±è´¥: {e}")
                
                payload = {
                    'content': content,
                    'user_id': user_id,
                    'importance': importance,
                    'memory_type': memory_type,
                    'tags': tags,
                    'entity_ids': entity_ids,
                    'created_at': datetime.now().isoformat(),
                    'processed': False
                }
                
                if qdrant_client and qdrant_client.is_available():
                    qdrant_client.add_memory(memory_id, vector, payload)
                    # æ›´æ–° BM25 ç´¢å¼•
                    update_bm25_index(memory_id, content)
                
                added_count += 1
                type_counts[memory_type] = type_counts.get(memory_type, 0) + 1
        
        result = {
            "status": "success",
            "message": f"ç›´æ¥æ·»åŠ  {added_count} æ¡è®°å¿†",
            "added": added_count,
            "type_breakdown": type_counts
        }
        
        if extracted_entities:
            result["extracted_entities"] = extracted_entities
            result["entity_count"] = len(extracted_entities)
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/search")
async def search_memory(request: SearchMemoryRequest):
    """æœç´¢è®°å¿†
    
    æ”¯æŒåŠŸèƒ½ï¼š
    - å‘é‡è¯­ä¹‰æœç´¢
    - BM25å…³é”®è¯æœç´¢ï¼ˆå¯é€‰ï¼Œæ··åˆæ£€ç´¢ï¼‰
    - é‡è¦åº¦åŠ æƒ
    - è®°å¿†ç±»å‹åŠ æƒï¼ˆåå¥½è®°å¿†æƒé‡æ›´é«˜ï¼‰
    - å›¾å¢å¼ºæœç´¢ï¼ˆå¯é€‰ï¼‰
    - æ ‡ç­¾è¿‡æ»¤
    - è®°å¿†ç±»å‹è¿‡æ»¤
    """
    if not embedding_model:
        raise HTTPException(status_code=500, detail="Embedding æ¨¡å‹æœªåŠ è½½")
    
    try:
        user_id = request.user_id or USER_ID
        
        # ç¡®å®šæ˜¯å¦å¯ç”¨ BM25ï¼ˆè¯·æ±‚å‚æ•°ä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ï¼‰
        enable_bm25 = request.use_bm25
        if enable_bm25 is None:
            enable_bm25 = config.get('search', {}).get('enable_bm25', False) if config else False
        
        # ç¡®å®šæ˜¯å¦å¯ç”¨å›¾å¢å¼ºï¼ˆè¯·æ±‚å‚æ•°ä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ï¼‰
        enable_graph = request.use_graph
        if enable_graph is None:
            enable_graph = config.get('search', {}).get('enable_graph_query', False) if config else False
        
        # ğŸ” è¢«åŠ¨æ£€ç´¢æ—¥å¿—
        print(f"\n{'='*50}")
        print(f"ğŸ” [è¢«åŠ¨æ£€ç´¢] æŸ¥è¯¢: {request.query[:80]}{'...' if len(request.query) > 80 else ''}")
        print(f"   å‚æ•°: top_k={request.top_k}, é˜ˆå€¼={request.similarity_threshold}, å›¾è°±={'å¯ç”¨' if enable_graph else 'ç¦ç”¨'}, BM25={'å¯ç”¨' if enable_bm25 else 'ç¦ç”¨'}")
        
        query_vector = encode_text(request.query)
        
        # ä½¿ç”¨å­—å…¸æ¥åˆå¹¶ä¸åŒæ£€ç´¢æ–¹å¼çš„ç»“æœ
        results_map = {}  # id -> {data, scores: {vector, bm25}}
        
        # 1. Qdrant å‘é‡æœç´¢
        if qdrant_client and qdrant_client.is_available():
            vector_results = qdrant_client.search(
                query_vector=query_vector,
                top_k=request.top_k * 3,  # å¤šå–ä¸€äº›ç”¨äºè¿‡æ»¤å’Œé‡æ’
                score_threshold=request.similarity_threshold,
                user_id=user_id
            )
            for r in vector_results:
                r_id = r.get('id')
                if r_id:
                    results_map[r_id] = {
                        'data': r,
                        'scores': {'vector': r.get('similarity', 0)}
                    }
        
        # 2. BM25 å…³é”®è¯æœç´¢
        if enable_bm25 and bm25_searcher:
            try:
                bm25_results = bm25_searcher.search(request.query, top_k=request.top_k * 3)
                
                if bm25_results:
                    # å½’ä¸€åŒ– BM25 åˆ†æ•°
                    max_bm25_score = max(score for _, score in bm25_results) or 1
                    
                    for doc_id, score in bm25_results:
                        normalized_score = score / max_bm25_score
                        
                        if doc_id in results_map:
                            # å·²å­˜åœ¨äºå‘é‡æœç´¢ç»“æœä¸­ï¼Œæ·»åŠ  BM25 åˆ†æ•°
                            results_map[doc_id]['scores']['bm25'] = normalized_score
                        else:
                            # ä»… BM25 æ‰¾åˆ°çš„ç»“æœï¼Œéœ€è¦ä»å­˜å‚¨è·å–å®Œæ•´æ•°æ®
                            memory = qdrant_client.get_memory(doc_id) if qdrant_client else None
                            if memory and memory.get('payload', {}).get('user_id') == user_id:
                                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                                memory_data = {
                                    'id': doc_id,
                                    'content': memory.get('payload', {}).get('content', memory.get('content', '')),
                                    'similarity': 0,  # æ— å‘é‡ç›¸ä¼¼åº¦
                                    'importance': memory.get('payload', {}).get('importance', memory.get('importance', 0.5)),
                                    'memory_type': memory.get('payload', {}).get('memory_type', memory.get('memory_type', 'general')),
                                    'tags': memory.get('payload', {}).get('tags', memory.get('tags', [])),
                                    'created_at': memory.get('payload', {}).get('created_at', memory.get('created_at')),
                                    'updated_at': memory.get('payload', {}).get('updated_at', memory.get('updated_at')),
                                    'entity_ids': memory.get('payload', {}).get('entity_ids', memory.get('entity_ids', [])),
                                    'bm25_only': True  # æ ‡è®°ä»…BM25æ‰¾åˆ°
                                }
                                results_map[doc_id] = {
                                    'data': memory_data,
                                    'scores': {'bm25': normalized_score}
                                }
                
                print(f"   ğŸ“Š BM25 æ‰¾åˆ° {len(bm25_results)} æ¡å€™é€‰è®°å¿†")
            except Exception as e:
                print(f"[è­¦å‘Š] BM25 æœç´¢å¤±è´¥: {e}")
        
        # 3. åˆå¹¶ç»“æœå¹¶è®¡ç®—æ··åˆå¾—åˆ†
        bm25_weight = config.get('search', {}).get('bm25_weight', 0.3) if config else 0.3
        vector_weight = 1.0 - bm25_weight if enable_bm25 else 1.0
        
        results = []
        for r_id, r_data in results_map.items():
            result = r_data['data'].copy()
            scores = r_data['scores']
            
            # è®¡ç®—æ··åˆç›¸ä¼¼åº¦å¾—åˆ†
            if enable_bm25:
                vector_score = scores.get('vector', 0)
                bm25_score = scores.get('bm25', 0)
                # æ··åˆå¾—åˆ† = å‘é‡æƒé‡ * å‘é‡å¾—åˆ† + BM25æƒé‡ * BM25å¾—åˆ†
                mixed_similarity = vector_weight * vector_score + bm25_weight * bm25_score
                result['similarity'] = mixed_similarity
                result['vector_score'] = vector_score
                result['bm25_score'] = bm25_score
            
            results.append(result)
        
        # æ ‡ç­¾è¿‡æ»¤
        if request.tags:
            results = [
                r for r in results 
                if any(tag in r.get('tags', []) for tag in request.tags)
            ]
        
        # è®°å¿†ç±»å‹è¿‡æ»¤
        if request.memory_types:
            results = [
                r for r in results 
                if r.get('memory_type', 'general') in request.memory_types
            ]
        
        # å›¾å¢å¼ºæœç´¢
        if enable_graph:
            if not neo4j_client:
                print(f"   âš ï¸ å›¾è°±å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            elif not neo4j_client.is_available():
                print(f"   âš ï¸ å›¾è°±å®¢æˆ·ç«¯ä¸å¯ç”¨")
            else:
                # ä»æŸ¥è¯¢ä¸­æå–å®ä½“è¿›è¡Œå›¾è°±å…³è”æœç´¢
                try:
                    import re
                    # æå–æŸ¥è¯¢ä¸­çš„æ½œåœ¨å®ä½“å
                    potential_entities = []
                    # ä¸­æ–‡è¯æ±‡
                    potential_entities.extend(re.findall(r'[\u4e00-\u9fff]{2,4}', request.query))
                    # è‹±æ–‡ä¸“æœ‰åè¯
                    potential_entities.extend(re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*', request.query))
                    
                    print(f"   ğŸ•¸ï¸ å›¾è°±æœç´¢: æå–åˆ° {len(potential_entities)} ä¸ªå€™é€‰å®ä½“")
                    
                    # åŒ¹é…å›¾ä¸­çš„å®ä½“
                    matched_entity_ids = []
                    for name in potential_entities[:10]:
                        entity = neo4j_client.find_entity_by_name(name, user_id)
                        if entity:
                            matched_entity_ids.append(entity['id'])
                            # è·å–ç›¸å…³å®ä½“
                            related = neo4j_client.find_related_entities(entity['id'], max_depth=2)
                            for rel in related:
                                if rel['id'] not in matched_entity_ids:
                                    matched_entity_ids.append(rel['id'])
                    
                    if matched_entity_ids:
                        print(f"   ğŸ•¸ï¸ å›¾è°±æœç´¢: åŒ¹é…åˆ° {len(matched_entity_ids)} ä¸ªå®ä½“")
                        # è·å–å®ä½“å…³è”çš„è®°å¿†
                        if hasattr(neo4j_client, 'get_memories_by_entities'):
                            graph_memory_ids = neo4j_client.get_memories_by_entities(matched_entity_ids)
                        else:
                            graph_memory_ids = []
                            for eid in matched_entity_ids:
                                mids = neo4j_client.get_entity_memories(eid)
                                graph_memory_ids.extend(mids)
                            graph_memory_ids = list(set(graph_memory_ids))
                        
                        # ä¸ºå›¾è°±å…³è”çš„è®°å¿†æ·»åŠ åŠ åˆ†
                        result_ids = {r.get('id') for r in results}
                        graph_boost_count = 0
                        for r in results:
                            r_id = r.get('id')
                            r_entities = r.get('entity_ids', [])
                            
                            # æ£€æŸ¥æ˜¯å¦é€šè¿‡å›¾è°±å…³è”
                            if r_id in graph_memory_ids:
                                r['graph_boost'] = 0.15  # ç›´æ¥å…³è”åŠ åˆ†
                                graph_boost_count += 1
                            elif any(eid in r_entities for eid in matched_entity_ids):
                                r['graph_boost'] = 0.1  # å®ä½“åŒ¹é…åŠ åˆ†
                                graph_boost_count += 1
                        
                        # æ·»åŠ å‘é‡æœç´¢æœªæ‰¾åˆ°ä½†å›¾è°±å…³è”çš„è®°å¿†
                        graph_only_count = 0
                        for mem_id in graph_memory_ids[:5]:
                            if mem_id not in result_ids:
                                memory = qdrant_client.get_memory(mem_id)
                                if memory and memory.get('payload', {}).get('user_id') == user_id:
                                    memory['graph_boost'] = 0.2
                                    memory['similarity'] = 0.5  # åŸºç¡€ç›¸ä¼¼åº¦
                                    memory['graph_only'] = True
                                    results.append(memory)
                                    graph_only_count += 1
                        
                        if graph_boost_count > 0 or graph_only_count > 0:
                            print(f"   ğŸ•¸ï¸ å›¾è°±å¢å¼º: {graph_boost_count} æ¡åŠ åˆ†, {graph_only_count} æ¡ä»…å›¾è°±")
                    else:
                        print(f"   ğŸ•¸ï¸ å›¾è°±æœç´¢: æœªåŒ¹é…åˆ°ä»»ä½•å®ä½“")
                                
                except Exception as e:
                    print(f"[è­¦å‘Š] å›¾å¢å¼ºæœç´¢å¤±è´¥: {e}")
        
        # åº”ç”¨å¤šç»´åŠ æƒ
        IMPORTANCE_WEIGHT = config.get('search', {}).get('importance_weight', 0.3) if config else 0.3
        TYPE_WEIGHT_FACTOR = config.get('search', {}).get('type_weight_factor', 0.2) if config else 0.2
        
        for result in results:
            similarity = result.get('similarity', 0)
            importance = result.get('importance', 0.5)
            memory_type = result.get('memory_type', 'general')
            graph_boost = result.get('graph_boost', 0)
            
            # è·å–ç±»å‹æƒé‡
            type_weight = MEMORY_TYPE_WEIGHTS.get(memory_type, 1.0)
            
            # ç»¼åˆå¾—åˆ† = ç›¸ä¼¼åº¦ Ã— (1 + é‡è¦åº¦åŠ æƒ + ç±»å‹åŠ æƒ) + å›¾è°±åŠ åˆ†
            result['final_score'] = similarity * (1 + importance * IMPORTANCE_WEIGHT + (type_weight - 1) * TYPE_WEIGHT_FACTOR) + graph_boost
            result['type_weight'] = type_weight
            result['graph_boost'] = graph_boost
        
        # æ’åº
        results.sort(key=lambda x: x.get('final_score', 0), reverse=True)
        results = results[:request.top_k]
        
        # æ ¼å¼åŒ–è¿”å›
        formatted_results = []
        for r in results:
            item = {
                "content": r.get('content', ''),
                "similarity": round(r.get('similarity', 0), 4),
                "importance": r.get('importance', 0.5),
                "memory_type": r.get('memory_type', 'general'),
                "tags": r.get('tags', []),
                "type_weight": r.get('type_weight', 1.0),
                "graph_boost": round(r.get('graph_boost', 0), 4),
                "final_score": round(r.get('final_score', 0), 4),
                "timestamp": r.get('created_at'),
                "created_at": r.get('created_at'),
                "updated_at": r.get('updated_at')
            }
            # æ·»åŠ  BM25 ç›¸å…³å­—æ®µï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_bm25:
                item["vector_score"] = round(r.get('vector_score', 0), 4)
                item["bm25_score"] = round(r.get('bm25_score', 0), 4)
                if r.get('bm25_only'):
                    item["bm25_only"] = True
            formatted_results.append(item)
        
        # ğŸ” æ£€ç´¢ç»“æœæ—¥å¿—ï¼ˆç»¼åˆæ£€ç´¢è¯¦æƒ…ï¼‰
        if formatted_results:
            print(f"âœ… [æ£€ç´¢ç»“æœ] æ‰¾åˆ° {len(formatted_results)} æ¡ç›¸å…³è®°å¿†:")
            for i, mem in enumerate(formatted_results[:5]):  # æœ€å¤šæ˜¾ç¤º5æ¡
                content_preview = mem['content'][:50].replace('\n', ' ')
                type_label = {'preference': 'åå¥½', 'fact': 'äº‹å®', 'episodic': 'æƒ…æ™¯', 
                             'semantic': 'è¯­ä¹‰', 'procedural': 'ç¨‹åºæ€§', 'general': 'é€šç”¨'}.get(mem['memory_type'], mem['memory_type'])
                # ç»¼åˆå¾—åˆ†è¯¦æƒ…
                graph_info = f"|å›¾è°±:{mem.get('graph_boost', 0):.2f}" if mem.get('graph_boost', 0) > 0 else ""
                bm25_info = ""
                if enable_bm25:
                    bm25_info = f"|å‘é‡:{mem.get('vector_score', 0):.2f}|BM25:{mem.get('bm25_score', 0):.2f}"
                    if mem.get('bm25_only'):
                        bm25_info += "(ä»…BM25)"
                print(f"   {i+1}. [{type_label}] {content_preview}...")
                print(f"      â””â”€ ç›¸ä¼¼åº¦:{mem['similarity']:.2f}{bm25_info} | ç±»å‹æƒé‡:{mem['type_weight']:.1f}x | æœ€ç»ˆå¾—åˆ†:{mem['final_score']:.2f}{graph_info}")
            if len(formatted_results) > 5:
                print(f"   ... è¿˜æœ‰ {len(formatted_results) - 5} æ¡")
        else:
            print(f"â„¹ï¸ [æ£€ç´¢ç»“æœ] æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
        print(f"{'='*50}\n")
        
        return {
            "query": request.query,
            "memories": formatted_results,
            "count": len(formatted_results)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/list")
async def list_memories(user_id: Optional[str] = USER_ID, limit: int = 100):
    """åˆ—å‡ºè®°å¿†"""
    try:
        memories = []
        
        if qdrant_client and qdrant_client.is_available():
            memories = qdrant_client.get_all_memories(user_id=user_id, limit=limit)
        
        results = [
            {
                "id": mem.get('id', ''),
                "content": mem.get('content', ''),
                "timestamp": mem.get('created_at'),
                "created_at": mem.get('created_at'),
                "updated_at": mem.get('updated_at'),
                "importance": mem.get('importance', 0.5),
                "merge_count": mem.get('merge_count', 0),
                "memory_type": mem.get('memory_type', 'general'),
                "tags": mem.get('tags', [])  # æ·»åŠ æ ‡ç­¾å­—æ®µ
            }
            for mem in memories
        ]
        
        return {
            "user_id": user_id,
            "count": len(results),
            "memories": results
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/delete/{memory_id}")
async def delete_memory(memory_id: str):
    """åˆ é™¤è®°å¿†"""
    try:
        if qdrant_client and qdrant_client.is_available():
            success = qdrant_client.delete_memory(memory_id)
            if success:
                return {"status": "success", "message": f"è®°å¿† {memory_id} å·²åˆ é™¤"}
        
        raise HTTPException(status_code=404, detail=f"è®°å¿† {memory_id} ä¸å­˜åœ¨")
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/stats")
async def get_statistics():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = {
            "total_count": 0,
            "today_count": 0,
            "week_count": 0,
            "avg_importance": 0,
            "storage_type": "qdrant" if qdrant_client else "memory",
            "graph_enabled": neo4j_client is not None
        }
        
        if qdrant_client and qdrant_client.is_available():
            info = qdrant_client.get_collection_info()
            stats["total_count"] = info.get('points_count', 0)
        
        if neo4j_client and neo4j_client.is_available():
            graph_stats = neo4j_client.get_stats()
            stats["entity_count"] = graph_stats.get('entity_count', 0)
            stats["relation_count"] = graph_stats.get('relation_count', 0)
        
        return stats
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


async def merge_memories_v2(keeper_id: str, content_a: str, content_b: str) -> bool:
    """ä½¿ç”¨ LLM æ™ºèƒ½åˆå¹¶ä¸¤æ¡ç›¸ä¼¼è®°å¿†ï¼ˆé€‚é… Qdrant å­˜å‚¨ï¼‰
    
    å°†ä¸¤æ¡è®°å¿†çš„å†…å®¹äº¤ç»™ LLM èåˆï¼Œä¿ç•™åŒæ–¹çš„ç‹¬ç‰¹ä¿¡æ¯ï¼Œå»é™¤é‡å¤éƒ¨åˆ†ã€‚
    åˆå¹¶åæ›´æ–°ä¿ç•™æ–¹çš„ content å’Œ vectorã€‚
    
    Args:
        keeper_id: è¦ä¿ç•™çš„è®°å¿† IDï¼ˆåˆå¹¶åçš„å†…å®¹å†™å…¥è¿™æ¡ï¼‰
        content_a: ç¬¬ä¸€æ¡è®°å¿†çš„å†…å®¹
        content_b: ç¬¬äºŒæ¡è®°å¿†çš„å†…å®¹
    
    Returns:
        True è¡¨ç¤ºåˆå¹¶æˆåŠŸï¼ŒFalse è¡¨ç¤ºå¤±è´¥ï¼ˆä¸¤æ¡å‡åº”ä¿ç•™ï¼‰
    """
    global llm_config
    
    if not llm_config:
        print("âš ï¸ LLM æœªé…ç½®ï¼Œæ— æ³•åˆå¹¶è®°å¿†")
        return False
    
    import aiohttp
    
    api_key = llm_config.get('api_key', '')
    model = llm_config.get('model', '')
    base_url = llm_config.get('base_url', '')
    
    if not all([api_key, model, base_url]):
        print("âš ï¸ LLM é…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•åˆå¹¶è®°å¿†")
        return False
    
    prompt = f"""åˆå¹¶ä»¥ä¸‹ä¸¤æ¡ç›¸ä¼¼çš„è®°å¿†ï¼Œä¿ç•™æ‰€æœ‰æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œå»é™¤é‡å¤å†…å®¹ï¼š

å·²æœ‰è®°å¿†ï¼š{content_a}
æ–°å¢ä¿¡æ¯ï¼š{content_b}

åˆå¹¶åçš„è®°å¿†ï¼ˆä¿ç•™æ‰€æœ‰ç»†èŠ‚ï¼Œç”¨åˆ†å·åˆ†éš”è¦ç‚¹ï¼‰ï¼š"""
    
    # é‡è¯•æœºåˆ¶ï¼šæœ€å¤š 3 æ¬¡ï¼Œè¶…æ—¶é€æ¬¡å¢åŠ 
    max_retries = 3
    timeouts = [60, 90, 120]
    
    for attempt in range(max_retries):
        try:
            timeout_seconds = timeouts[attempt]
            
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
                
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 2000,
                    "temperature": 0.2
                }
                
                async with session.post(
                    f"{base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=timeout_seconds)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        merged_content = result['choices'][0]['message']['content'].strip()
                        
                        # ç”Ÿæˆæ–°çš„ embedding
                        new_vector = encode_text(merged_content)
                        
                        # è·å–ä¿ç•™æ–¹çš„å½“å‰ payload
                        full_mem = qdrant_client.get_memory(keeper_id)
                        if not full_mem:
                            print(f"âš ï¸ æ‰¾ä¸åˆ°è®°å¿† {keeper_id}ï¼Œåˆå¹¶å¤±è´¥")
                            return False
                        
                        keeper_payload = full_mem.get('payload', {})
                        
                        # æ›´æ–° payload
                        keeper_payload['content'] = merged_content
                        keeper_payload['updated_at'] = datetime.now().isoformat()
                        keeper_payload['merge_count'] = keeper_payload.get('merge_count', 0) + 1
                        
                        # å†™å…¥ Qdrant
                        qdrant_client.update_memory(keeper_id, keeper_payload, new_vector)
                        
                        # æ›´æ–° BM25 ç´¢å¼•
                        update_bm25_index(keeper_id, merged_content)
                        
                        print(f"   ğŸ¤– LLMåˆå¹¶æˆåŠŸ (ç¬¬ {keeper_payload['merge_count']} æ¬¡): {merged_content[:50]}...")
                        return True
                    else:
                        error_text = await response.text()
                        print(f"âš ï¸ LLM API è¿”å›é”™è¯¯ {response.status}: {error_text[:200]}")
                        return False  # API é”™è¯¯ä¸é‡è¯•
                        
        except asyncio.TimeoutError:
            print(f"âš ï¸ LLM åˆå¹¶è¶…æ—¶ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡, {timeouts[attempt]}ç§’)")
            if attempt < max_retries - 1:
                print(f"   ğŸ”„ ç­‰å¾… 5 ç§’åé‡è¯•...")
                await asyncio.sleep(5)
        except Exception as e:
            print(f"âš ï¸ LLM åˆå¹¶å¼‚å¸¸: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(3)
    
    print(f"âŒ LLM åˆå¹¶å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰ï¼Œä¸¤æ¡è®°å¿†å‡ä¿ç•™")
    return False


@app.post("/deduplicate")
async def deduplicate_memories(
    threshold: float = 0.90,
    by_type: bool = True  # ğŸ”¥ æ–°å¢ï¼šæ˜¯å¦æŒ‰è®°å¿†ç±»å‹åˆ†ç»„å»é‡
):
    """å»é‡ï¼ˆæ”¯æŒæŒ‰è®°å¿†ç±»å‹åˆ†ç»„ï¼‰
    
    Args:
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤ 0.90
        by_type: æ˜¯å¦æŒ‰ memory_type åˆ†ç»„å»é‡ï¼Œé»˜è®¤ True
                 - True: åªåœ¨åŒç±»å‹è®°å¿†ä¹‹é—´å»é‡ï¼ˆæ¨èï¼Œé¿å…ä¸åŒç±»å‹è®°å¿†è¢«é”™è¯¯åˆå¹¶ï¼‰
                 - False: å…¨å±€å»é‡ï¼ˆæ‰€æœ‰è®°å¿†ä¹‹é—´æ¯”è¾ƒï¼‰
    """
    try:
        if not qdrant_client or not qdrant_client.is_available():
            return {"status": "error", "message": "å­˜å‚¨ä¸å¯ç”¨"}
        
        # è·å–æ‰€æœ‰è®°å¿†
        memories = qdrant_client.get_all_memories(limit=10000)
        
        if len(memories) < 2:
            return {"status": "success", "merged_count": 0, "by_type": by_type}
        
        deleted_ids = set()
        merged_count = 0
        type_stats = {}  # è®°å½•æ¯ä¸ªç±»å‹çš„å»é‡ç»Ÿè®¡
        
        if by_type:
            # ğŸ”¥ æŒ‰ memory_type åˆ†ç»„
            type_groups = {}
            for mem in memories:
                mem_type = mem.get('memory_type', 'general')
                type_groups.setdefault(mem_type, []).append(mem)
            
            print(f"ğŸ” æŒ‰ç±»å‹åˆ†ç»„å»é‡ï¼ˆé˜ˆå€¼: {threshold}ï¼‰")
            for mem_type, group in type_groups.items():
                print(f"   ğŸ“ {mem_type}: {len(group)} æ¡è®°å¿†")
            
            # åœ¨æ¯ä¸ªç±»å‹ç»„å†…å»é‡
            for mem_type, group in type_groups.items():
                if len(group) < 2:
                    continue
                
                group_deleted = 0
                
                for i, mem_i in enumerate(group):
                    if mem_i['id'] in deleted_ids:
                        continue
                    
                    full_mem_i = qdrant_client.get_memory(mem_i['id'])
                    if not full_mem_i or 'vector' not in full_mem_i:
                        continue
                    
                    emb_i = np.array(full_mem_i['vector'])
                    
                    for j in range(i + 1, len(group)):
                        mem_j = group[j]
                        if mem_j['id'] in deleted_ids:
                            continue
                        
                        full_mem_j = qdrant_client.get_memory(mem_j['id'])
                        if not full_mem_j or 'vector' not in full_mem_j:
                            continue
                        
                        emb_j = np.array(full_mem_j['vector'])
                        
                        similarity = float(cosine_similarity([emb_i], [emb_j])[0][0])
                        
                        if similarity >= threshold:
                            print(f"   ğŸ”— [{mem_type}] å‘ç°ç›¸ä¼¼è®°å¿† (ç›¸ä¼¼åº¦: {similarity:.2%})")
                            print(f"      è®°å¿†1: {mem_i.get('content', '')[:50]}...")
                            print(f"      è®°å¿†2: {mem_j.get('content', '')[:50]}...")
                            
                            # ä½¿ç”¨ LLM æ™ºèƒ½åˆå¹¶ï¼ˆä¿ç•™åŒæ–¹ç‹¬ç‰¹ä¿¡æ¯ï¼‰
                            merge_success = await merge_memories_v2(
                                keeper_id=mem_i['id'],
                                content_a=mem_i.get('content', ''),
                                content_b=mem_j.get('content', '')
                            )
                            
                            if merge_success:
                                deleted_ids.add(mem_j['id'])
                                merged_count += 1
                                group_deleted += 1
                            else:
                                print(f"   â­ï¸ [{mem_type}] LLMåˆå¹¶å¤±è´¥ï¼Œä¸¤æ¡å‡ä¿ç•™")
                
                if group_deleted > 0:
                    type_stats[mem_type] = group_deleted
        else:
            # ğŸ”¥ åŸæœ‰å…¨å±€å»é‡é€»è¾‘
            print(f"ğŸ” å…¨å±€å»é‡ï¼ˆé˜ˆå€¼: {threshold}ï¼‰")
            
            for i, mem_i in enumerate(memories):
                if mem_i['id'] in deleted_ids:
                    continue
                
                full_mem_i = qdrant_client.get_memory(mem_i['id'])
                if not full_mem_i or 'vector' not in full_mem_i:
                    continue
                
                emb_i = np.array(full_mem_i['vector'])
                
                for j in range(i + 1, len(memories)):
                    mem_j = memories[j]
                    if mem_j['id'] in deleted_ids:
                        continue
                    
                    full_mem_j = qdrant_client.get_memory(mem_j['id'])
                    if not full_mem_j or 'vector' not in full_mem_j:
                        continue
                    
                    emb_j = np.array(full_mem_j['vector'])
                    
                    similarity = float(cosine_similarity([emb_i], [emb_j])[0][0])
                    
                    if similarity >= threshold:
                        print(f"   ğŸ”— å‘ç°ç›¸ä¼¼è®°å¿† (ç›¸ä¼¼åº¦: {similarity:.2%})")
                        print(f"      è®°å¿†1: {mem_i.get('content', '')[:50]}...")
                        print(f"      è®°å¿†2: {mem_j.get('content', '')[:50]}...")
                        
                        # ä½¿ç”¨ LLM æ™ºèƒ½åˆå¹¶ï¼ˆä¿ç•™åŒæ–¹ç‹¬ç‰¹ä¿¡æ¯ï¼‰
                        merge_success = await merge_memories_v2(
                            keeper_id=mem_i['id'],
                            content_a=mem_i.get('content', ''),
                            content_b=mem_j.get('content', '')
                        )
                        
                        if merge_success:
                            deleted_ids.add(mem_j['id'])
                            merged_count += 1
                        else:
                            print(f"   â­ï¸ LLMåˆå¹¶å¤±è´¥ï¼Œä¸¤æ¡å‡ä¿ç•™")
        
        # åˆ é™¤é‡å¤è®°å¿†
        if deleted_ids:
            qdrant_client.delete_memories_batch(list(deleted_ids))
        
        print(f"âœ… å»é‡å®Œæˆï¼åˆå¹¶ {merged_count} æ¡è®°å¿†")
        
        return {
            "status": "success",
            "merged_count": merged_count,
            "remaining_count": len(memories) - len(deleted_ids),
            "by_type": by_type,
            "type_stats": type_stats if by_type else None
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/reclassify")
async def reclassify_all_memories(
    dry_run: bool = False,
    limit: int = 10000
):
    """æ‰¹é‡é‡æ–°åˆ†ç±»æ‰€æœ‰å†å²è®°å¿†
    
    å°†æ¯æ¡ç°æœ‰è®°å¿†é‡æ–°ç”¨ LLM æ‹†åˆ†+åˆ†ç±»ï¼Œç”Ÿæˆæ–°çš„åˆ†ç±»è®°å¿†ï¼Œä¿ç•™åŸå§‹æ—¶é—´æˆ³ã€‚
    
    Args:
        dry_run: æ˜¯å¦åªé¢„è§ˆä¸æ‰§è¡Œï¼ˆTrue æ—¶åªè¿”å›é¢„è§ˆç»“æœï¼Œä¸ä¿®æ”¹æ•°æ®ï¼‰
        limit: å¤„ç†çš„æœ€å¤§è®°å¿†æ•°é‡
    
    æµç¨‹ï¼š
    1. è·å–æ‰€æœ‰å†å²è®°å¿†
    2. é€æ¡ç”¨ LLM é‡æ–°æå–å’Œåˆ†ç±»ï¼ˆå¤ç”¨ process_conversation_batchï¼‰
    3. æ–°è®°å¿†ç»§æ‰¿åŸå§‹ created_at æ—¶é—´æˆ³
    4. å»é‡åæ·»åŠ æ–°è®°å¿†ï¼Œåˆ é™¤åŸè®°å¿†
    """
    global qdrant_client, embedding_model
    
    if not qdrant_client or not qdrant_client.is_available():
        raise HTTPException(status_code=500, detail="å­˜å‚¨ä¸å¯ç”¨")
    
    if not embedding_model:
        raise HTTPException(status_code=500, detail="Embedding æ¨¡å‹æœªåŠ è½½")
    
    try:
        # 1. è·å–æ‰€æœ‰è®°å¿†
        all_memories = qdrant_client.get_all_memories(limit=limit)
        total_original = len(all_memories)
        
        if total_original == 0:
            return {"status": "success", "message": "æ²¡æœ‰è®°å¿†éœ€è¦å¤„ç†"}
        
        print(f"\n{'='*60}")
        print(f"ğŸ”„ [é‡æ–°åˆ†ç±»] å¼€å§‹å¤„ç† {total_original} æ¡å†å²è®°å¿†...")
        print(f"{'='*60}")
        
        new_memories_to_add = []  # å¾…æ·»åŠ çš„æ–°è®°å¿†
        original_ids_to_delete = []  # å¾…åˆ é™¤çš„åŸè®°å¿† ID
        failed_count = 0
        skipped_count = 0
        type_distribution = {}
        
        # 2. é€æ¡å¤„ç†
        for idx, mem in enumerate(all_memories):
            original_id = mem['id']
            original_content = mem.get('content', '')
            original_time = mem.get('created_at') or mem.get('timestamp', datetime.now().isoformat())
            original_importance = mem.get('importance', 0.5)
            user_id = mem.get('user_id', USER_ID)
            
            if not original_content or len(original_content) < 5:
                skipped_count += 1
                continue
            
            print(f"\nğŸ“ [{idx+1}/{total_original}] å¤„ç†: {original_content[:50]}...")
            
            try:
                # è°ƒç”¨ç°æœ‰çš„ LLM æå–å‡½æ•°
                result = await process_conversation_batch(original_content)
                extracted_memories = result.get('memories', [])
                
                if not extracted_memories:
                    # æå–å¤±è´¥ï¼Œä¿ç•™åŸè®°å¿†
                    print(f"   âš ï¸ æå–å¤±è´¥ï¼Œä¿ç•™åŸè®°å¿†")
                    failed_count += 1
                    continue
                
                # å¤„ç†æå–å‡ºçš„æ¯æ¡æ–°è®°å¿†
                for new_mem in extracted_memories:
                    new_content = new_mem.get('content', '').strip()
                    if not new_content or len(new_content) < 5:
                        continue
                    
                    memory_type = new_mem.get('memory_type', 'general')
                    tags = new_mem.get('tags', [])
                    importance = new_mem.get('importance', original_importance)
                    
                    # ç»Ÿè®¡ç±»å‹åˆ†å¸ƒ
                    type_distribution[memory_type] = type_distribution.get(memory_type, 0) + 1
                    
                    new_memories_to_add.append({
                        'content': new_content,
                        'memory_type': memory_type,
                        'tags': tags,
                        'importance': importance,
                        'created_at': original_time,  # ğŸ”¥ ç»§æ‰¿åŸå§‹æ—¶é—´æˆ³
                        'user_id': user_id,
                        'original_id': original_id,  # è®°å½•æ¥æº
                        'reclassified': True
                    })
                    
                    type_label = {'preference': 'åå¥½', 'fact': 'äº‹å®', 'episodic': 'æƒ…æ™¯', 
                                 'semantic': 'è¯­ä¹‰', 'procedural': 'ç¨‹åºæ€§', 'general': 'é€šç”¨'}.get(memory_type, memory_type)
                    print(f"   âœ… [{type_label}] {new_content[:40]}...")
                
                # æ ‡è®°åŸè®°å¿†å¾…åˆ é™¤
                original_ids_to_delete.append(original_id)
                
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                failed_count += 1
                continue
        
        # 3. é¢„è§ˆæ¨¡å¼ - åªè¿”å›ç»“æœä¸æ‰§è¡Œ
        if dry_run:
            print(f"\n{'='*60}")
            print(f"ğŸ” [é¢„è§ˆæ¨¡å¼] ä¸æ‰§è¡Œå®é™…ä¿®æ”¹")
            print(f"{'='*60}")
            
            return {
                "status": "preview",
                "dry_run": True,
                "original_count": total_original,
                "will_delete": len(original_ids_to_delete),
                "will_add": len(new_memories_to_add),
                "failed": failed_count,
                "skipped": skipped_count,
                "type_distribution": type_distribution,
                "sample_new_memories": [
                    {
                        "content": m['content'][:100],
                        "memory_type": m['memory_type'],
                        "tags": m['tags'],
                        "created_at": m['created_at']
                    } for m in new_memories_to_add[:10]  # é¢„è§ˆå‰ 10 æ¡
                ]
            }
        
        # 4. æ‰§è¡Œæ¨¡å¼ - æ·»åŠ æ–°è®°å¿†å¹¶åˆ é™¤åŸè®°å¿†
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ [æ‰§è¡Œ] å¼€å§‹å†™å…¥æ–°è®°å¿†...")
        print(f"{'='*60}")
        
        added_count = 0
        duplicate_skipped = 0
        
        for new_mem in new_memories_to_add:
            content = new_mem['content']
            vector = encode_text(content)
            
            # å»é‡æ£€æŸ¥
            similar = qdrant_client.find_similar(vector, threshold=0.95, user_id=new_mem['user_id'])
            if similar:
                duplicate_skipped += 1
                continue
            
            # æ·»åŠ æ–°è®°å¿†
            memory_id = str(uuid.uuid4())
            payload = {
                'content': content,
                'user_id': new_mem['user_id'],
                'importance': new_mem['importance'],
                'memory_type': new_mem['memory_type'],
                'tags': new_mem['tags'],
                'created_at': new_mem['created_at'],  # ä½¿ç”¨åŸå§‹æ—¶é—´
                'merge_count': 0,
                'processed': True,
                'reclassified': True,
                'original_id': new_mem.get('original_id')
            }
            
            qdrant_client.add_memory(memory_id, vector, payload)
            added_count += 1
        
        # åˆ é™¤åŸè®°å¿†
        if original_ids_to_delete:
            print(f"\nğŸ—‘ï¸ åˆ é™¤ {len(original_ids_to_delete)} æ¡åŸè®°å¿†...")
            qdrant_client.delete_memories_batch(original_ids_to_delete)
        
        print(f"\n{'='*60}")
        print(f"âœ… [å®Œæˆ] é‡æ–°åˆ†ç±»å®Œæˆï¼")
        print(f"   åŸè®°å¿†: {total_original} æ¡")
        print(f"   æ–°è®°å¿†: {added_count} æ¡")
        print(f"   å»é‡è·³è¿‡: {duplicate_skipped} æ¡")
        print(f"   å¤±è´¥: {failed_count} æ¡")
        print(f"{'='*60}")
        
        return {
            "status": "success",
            "original_count": total_original,
            "deleted_count": len(original_ids_to_delete),
            "added_count": added_count,
            "duplicate_skipped": duplicate_skipped,
            "failed": failed_count,
            "skipped": skipped_count,
            "type_distribution": type_distribution
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"é‡æ–°åˆ†ç±»å¤±è´¥: {str(e)}")


@app.post("/extract-all-entities")
async def extract_entities_from_all_memories(
    dry_run: bool = False,
    limit: int = 10000
):
    """ä»æ‰€æœ‰è®°å¿†ä¸­æ‰¹é‡æå–å®ä½“å’Œå…³ç³»ï¼Œä¸°å¯ŒçŸ¥è¯†å›¾è°±
    
    Args:
        dry_run: æ˜¯å¦åªé¢„è§ˆä¸æ‰§è¡Œï¼ˆTrue æ—¶åªè¿”å›é¢„è§ˆç»“æœï¼Œä¸ä¿®æ”¹æ•°æ®ï¼‰
        limit: å¤„ç†çš„æœ€å¤§è®°å¿†æ•°é‡
    
    æµç¨‹ï¼š
    1. è·å–æ‰€æœ‰è®°å¿†
    2. é€æ¡ç”¨ LLM æå–å®ä½“å’Œå…³ç³»
    3. å»é‡åå­˜å…¥ Neo4j çŸ¥è¯†å›¾è°±
    """
    global qdrant_client, entity_extractor, neo4j_client
    
    if not entity_extractor:
        raise HTTPException(status_code=503, detail="å®ä½“æå–å™¨æœªå¯ç”¨ï¼ˆéœ€è¦é…ç½® entity_extraction.enabled=trueï¼‰")
    
    if not neo4j_client or not neo4j_client.is_available():
        raise HTTPException(status_code=503, detail="Neo4j çŸ¥è¯†å›¾è°±ä¸å¯ç”¨")
    
    if not qdrant_client or not qdrant_client.is_available():
        raise HTTPException(status_code=500, detail="å­˜å‚¨ä¸å¯ç”¨")
    
    try:
        # 1. è·å–æ‰€æœ‰è®°å¿†
        all_memories = qdrant_client.get_all_memories(limit=limit)
        total_memories = len(all_memories)
        
        if total_memories == 0:
            return {"status": "success", "message": "æ²¡æœ‰è®°å¿†éœ€è¦å¤„ç†"}
        
        print(f"\n{'='*60}")
        print(f"ğŸ•¸ï¸ [å®ä½“æå–] å¼€å§‹ä» {total_memories} æ¡è®°å¿†ä¸­æå–å®ä½“...")
        print(f"{'='*60}")
        
        entities_created = 0
        entities_skipped = 0
        relations_created = 0
        failed_count = 0
        entity_types_count = {}
        
        preview_entities = []  # é¢„è§ˆæ¨¡å¼ä¸‹æ”¶é›†çš„å®ä½“
        preview_relations = []  # é¢„è§ˆæ¨¡å¼ä¸‹æ”¶é›†çš„å…³ç³»
        
        user_id = USER_ID
        
        # 2. é€æ¡å¤„ç†
        for idx, mem in enumerate(all_memories):
            content = mem.get('content', '')
            
            if not content or len(content) < 5:
                continue
            
            print(f"\nğŸ“ [{idx+1}/{total_memories}] å¤„ç†: {content[:50]}...")
            
            try:
                # è°ƒç”¨å®ä½“æå–å™¨
                entities, relations = await entity_extractor.extract(content)
                
                if not entities:
                    print(f"   â„¹ï¸ æœªå‘ç°å®ä½“")
                    continue
                
                print(f"   å‘ç° {len(entities)} ä¸ªå®ä½“, {len(relations) if relations else 0} ä¸ªå…³ç³»")
                
                # å¤„ç†å®ä½“
                for entity in entities:
                    try:
                        entity_name = entity.name if hasattr(entity, 'name') else str(entity)
                        entity_type = entity.entity_type.value if hasattr(entity, 'entity_type') else 'unknown'
                        entity_desc = entity.description if hasattr(entity, 'description') else ''
                        
                        # ç»Ÿè®¡ç±»å‹åˆ†å¸ƒ
                        entity_types_count[entity_type] = entity_types_count.get(entity_type, 0) + 1
                        
                        if dry_run:
                            # é¢„è§ˆæ¨¡å¼ï¼šåªæ”¶é›†ä¸å­˜å‚¨
                            preview_entities.append({
                                "name": entity_name,
                                "type": entity_type,
                                "description": entity_desc,
                                "source": content[:50]
                            })
                            continue
                        
                        # æ£€æŸ¥å®ä½“æ˜¯å¦å·²å­˜åœ¨
                        existing = neo4j_client.find_entity_by_name(entity_name, user_id)
                        if existing:
                            entities_skipped += 1
                            print(f"   â­ï¸ å®ä½“å·²å­˜åœ¨: {entity_name}")
                            continue
                        
                        # åˆ›å»ºæ–°å®ä½“
                        new_entity_id = str(uuid.uuid4())
                        success = neo4j_client.create_entity(
                            entity_id=new_entity_id,
                            name=entity_name,
                            entity_type=entity_type,
                            user_id=user_id,
                            properties={'description': entity_desc} if entity_desc else {}
                        )
                        
                        if success:
                            entities_created += 1
                            print(f"   âœ… åˆ›å»ºå®ä½“: {entity_name} [{entity_type}]")
                        
                    except Exception as ee:
                        logger.warning(f"ä¿å­˜å®ä½“å¤±è´¥: {ee}")
                
                # å¤„ç†å…³ç³»
                if relations:
                    for rel in relations:
                        try:
                            source_name = rel.source_name if hasattr(rel, 'source_name') else ''
                            target_name = rel.target_name if hasattr(rel, 'target_name') else ''
                            relation_type = rel.relation_type.value if hasattr(rel, 'relation_type') else 'related_to'
                            rel_desc = rel.description if hasattr(rel, 'description') else ''
                            
                            if dry_run:
                                # é¢„è§ˆæ¨¡å¼
                                preview_relations.append({
                                    "source": source_name,
                                    "target": target_name,
                                    "type": relation_type,
                                    "description": rel_desc
                                })
                                continue
                            
                            # æŸ¥æ‰¾æºå’Œç›®æ ‡å®ä½“
                            source_entity = neo4j_client.find_entity_by_name(source_name, user_id)
                            target_entity = neo4j_client.find_entity_by_name(target_name, user_id)
                            
                            if source_entity and target_entity:
                                neo4j_client.create_relation(
                                    source_id=source_entity['id'],
                                    target_id=target_entity['id'],
                                    relation_type=relation_type,
                                    properties={'description': rel_desc} if rel_desc else {}
                                )
                                relations_created += 1
                                print(f"   ğŸ”— åˆ›å»ºå…³ç³»: {source_name} --[{relation_type}]--> {target_name}")
                                
                        except Exception as re:
                            logger.warning(f"ä¿å­˜å…³ç³»å¤±è´¥: {re}")
                
            except Exception as e:
                print(f"   âŒ æå–å¤±è´¥: {e}")
                failed_count += 1
                continue
        
        # 3. è¿”å›ç»“æœ
        if dry_run:
            print(f"\n{'='*60}")
            print(f"ğŸ” [é¢„è§ˆæ¨¡å¼] ä¸æ‰§è¡Œå®é™…ä¿®æ”¹")
            print(f"{'='*60}")
            
            return {
                "status": "preview",
                "dry_run": True,
                "memories_processed": total_memories,
                "entities_found": len(preview_entities),
                "relations_found": len(preview_relations),
                "entity_types": entity_types_count,
                "sample_entities": preview_entities[:20],  # é¢„è§ˆå‰ 20 ä¸ªå®ä½“
                "sample_relations": preview_relations[:10]  # é¢„è§ˆå‰ 10 ä¸ªå…³ç³»
            }
        
        print(f"\n{'='*60}")
        print(f"âœ… [å®Œæˆ] å®ä½“æå–å®Œæˆï¼")
        print(f"   å¤„ç†è®°å¿†: {total_memories} æ¡")
        print(f"   åˆ›å»ºå®ä½“: {entities_created} ä¸ª")
        print(f"   è·³è¿‡å·²å­˜åœ¨: {entities_skipped} ä¸ª")
        print(f"   åˆ›å»ºå…³ç³»: {relations_created} ä¸ª")
        print(f"   å¤±è´¥: {failed_count} æ¡")
        print(f"{'='*60}")
        
        return {
            "status": "success",
            "memories_processed": total_memories,
            "entities_created": entities_created,
            "entities_skipped": entities_skipped,
            "relations_created": relations_created,
            "failed": failed_count,
            "entity_types": entity_types_count
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å®ä½“æå–å¤±è´¥: {str(e)}")


@app.post("/migrate")
async def migrate_from_json(force: bool = False):
    """å°† JSON å¤‡ä»½æ•°æ®è¿ç§»åˆ° Qdrant
    
    Args:
        force: æ˜¯å¦å¼ºåˆ¶è¿ç§»ï¼ˆå³ä½¿ Qdrant å·²æœ‰æ•°æ®ï¼‰
    """
    global memory_store_backup, qdrant_client, embedding_model
    
    if not qdrant_client or not qdrant_client.is_available():
        return {"status": "error", "message": "Qdrant ä¸å¯ç”¨"}
    
    if not memory_store_backup:
        return {"status": "error", "message": "æ²¡æœ‰ JSON å¤‡ä»½æ•°æ®å¯è¿ç§»"}
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»
    current_count = qdrant_client.count_memories()
    if current_count > 0 and not force:
        return {
            "status": "skipped",
            "message": f"Qdrant å·²æœ‰ {current_count} æ¡æ•°æ®ï¼Œä½¿ç”¨ force=true å¼ºåˆ¶è¿ç§»",
            "json_count": len(memory_store_backup),
            "qdrant_count": current_count
        }
    
    # æ‰§è¡Œè¿ç§»
    batch = []
    migrated = 0
    skipped = 0
    
    for i, mem in enumerate(memory_store_backup):
        content = mem.get('content', '')
        if not content or len(content) < 5:
            skipped += 1
            continue
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ï¼‰
        existing = qdrant_client.search(
            query_vector=embedding_model.encode([content])[0].tolist(),
            top_k=1,
            user_id=mem.get('user_id', USER_ID)
        )
        if existing and existing[0].get('similarity', 0) > 0.95:
            skipped += 1
            continue
        
        # è·å–æˆ–ç”Ÿæˆå‘é‡
        if 'embedding' in mem and mem['embedding']:
            vector = mem['embedding']
        else:
            vector = embedding_model.encode([content])[0].tolist()
        
        # æ„å»º payload
        payload = {
            'content': content,
            'user_id': mem.get('user_id', USER_ID),
            'importance': mem.get('importance', 0.5),
            'memory_type': mem.get('memory_type', 'general'),
            'tags': mem.get('tags', []),
            'created_at': mem.get('created_at') or mem.get('timestamp', datetime.now().isoformat()),
            'updated_at': mem.get('updated_at'),
            'merge_count': mem.get('merge_count', 0),
            'source': 'migrated_from_json'
        }
        
        memory_id = str(uuid.uuid4())
        
        batch.append({
            'id': memory_id,
            'vector': vector,
            'payload': payload
        })
        
        if len(batch) >= 50:
            count = qdrant_client.add_memories_batch(batch)
            migrated += count
            batch = []
    
    if batch:
        count = qdrant_client.add_memories_batch(batch)
        migrated += count
    
    return {
        "status": "success",
        "message": f"è¿ç§»å®Œæˆ",
        "migrated": migrated,
        "skipped": skipped,
        "total_json": len(memory_store_backup),
        "new_qdrant_count": qdrant_client.count_memories()
    }


# ==================== åå¥½ç®¡ç†ç«¯ç‚¹ ====================

class AddPreferenceRequest(BaseModel):
    """æ·»åŠ åå¥½è¯·æ±‚"""
    item: str = Field(..., description="åå¥½å¯¹è±¡ï¼ˆå¦‚ï¼šç«é”…ã€è“è‰²ã€ç¼–ç¨‹ï¼‰")
    category: str = Field(default="other", description="ç±»åˆ«: food/color/hobby/music/movie/place/person/style/other")
    preference_type: str = Field(default="like", description="ç±»å‹: like/dislike")
    strength: float = Field(default=0.8, ge=0.0, le=1.0, description="åå¥½å¼ºåº¦")
    reason: Optional[str] = Field(default=None, description="åŸå› è¯´æ˜")
    user_id: Optional[str] = USER_ID


class ExtractPreferencesRequest(BaseModel):
    """ä»æ–‡æœ¬æå–åå¥½è¯·æ±‚"""
    text: str = Field(..., description="è¦æå–åå¥½çš„æ–‡æœ¬")
    user_id: Optional[str] = USER_ID


@app.get("/preferences")
async def get_preferences(
    user_id: str = Query(default=USER_ID),
    category: Optional[str] = Query(default=None, description="ç±»åˆ«è¿‡æ»¤"),
    preference_type: Optional[str] = Query(default=None, description="ç±»å‹è¿‡æ»¤: like/dislike")
):
    """è·å–ç”¨æˆ·åå¥½åˆ—è¡¨"""
    if not preference_memory:
        return {"preferences": [], "message": "åå¥½è®°å¿†æœªåˆå§‹åŒ–"}
    
    try:
        from memories.preference_memory import PreferenceCategory, PreferenceType
        
        cat = PreferenceCategory(category) if category else None
        ptype = PreferenceType(preference_type) if preference_type else None
        
        prefs = await preference_memory.get_preferences(category=cat, preference_type=ptype)
        
        return {
            "preferences": [p.dict() for p in prefs],
            "count": len(prefs)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/preferences")
async def add_preference(request: AddPreferenceRequest):
    """æ·»åŠ ç”¨æˆ·åå¥½"""
    if not preference_memory:
        raise HTTPException(status_code=503, detail="åå¥½è®°å¿†æœªåˆå§‹åŒ–")
    
    try:
        from memories.preference_memory import PreferenceCategory, PreferenceType
        
        pref = await preference_memory.add_preference(
            item=request.item,
            category=PreferenceCategory(request.category),
            preference_type=PreferenceType(request.preference_type),
            strength=request.strength
        )
        
        return {
            "status": "success",
            "preference": pref.dict(),
            "message": f"å·²æ·»åŠ åå¥½: {'å–œæ¬¢' if request.preference_type == 'like' else 'ä¸å–œæ¬¢'}{request.item}"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/preferences/{pref_id}")
async def delete_preference(pref_id: str):
    """åˆ é™¤åå¥½"""
    if not preference_memory:
        raise HTTPException(status_code=503, detail="åå¥½è®°å¿†æœªåˆå§‹åŒ–")
    
    try:
        success = await preference_memory.delete_preference(pref_id)
        if success:
            return {"status": "success", "message": "åå¥½å·²åˆ é™¤"}
        else:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°è¯¥åå¥½")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/preferences/summary")
async def get_preference_summary(user_id: str = Query(default=USER_ID)):
    """è·å–åå¥½æ‘˜è¦"""
    if not preference_memory:
        return {"summary": {}, "message": "åå¥½è®°å¿†æœªåˆå§‹åŒ–"}
    
    try:
        summary = await preference_memory.get_summary()
        return {"summary": summary}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/preferences/extract")
async def extract_preferences(request: ExtractPreferencesRequest):
    """ä»æ–‡æœ¬ä¸­æå–åå¥½ï¼ˆä½¿ç”¨ LLMï¼‰"""
    if not llm_config:
        raise HTTPException(status_code=503, detail="LLM æœªé…ç½®")
    
    try:
        from utils.entity_extractor import PreferenceExtractor
        
        # ä¼ é€’å¤‡ç”¨æ¨¡å‹é…ç½®
        fallback_cfg = full_config.get('llm_fallback', {}).get('config') if full_config else None
        extractor = PreferenceExtractor(llm_config, fallback_config=fallback_cfg)
        extracted = await extractor.extract_preferences(request.text)
        
        # è‡ªåŠ¨æ·»åŠ æå–åˆ°çš„åå¥½
        added_count = 0
        if preference_memory and extracted:
            from memories.preference_memory import PreferenceCategory, PreferenceType
            
            for like in extracted.get('likes', []):
                try:
                    cat = PreferenceCategory(like.get('category', 'other'))
                    await preference_memory.add_preference(
                        item=like.get('item', ''),
                        category=cat,
                        preference_type=PreferenceType.LIKE,
                        strength=like.get('strength', 0.8)
                    )
                    added_count += 1
                except:
                    pass
            
            for dislike in extracted.get('dislikes', []):
                try:
                    cat = PreferenceCategory(dislike.get('category', 'other'))
                    await preference_memory.add_preference(
                        item=dislike.get('item', ''),
                        category=cat,
                        preference_type=PreferenceType.DISLIKE,
                        strength=dislike.get('strength', 0.8)
                    )
                    added_count += 1
                except:
                    pass
        
        return {
            "status": "success",
            "extracted": extracted,
            "added_count": added_count
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/preferences/search")
async def search_preferences(query: str, user_id: str = Query(default=USER_ID), top_k: int = 5):
    """æœç´¢ç›¸å…³åå¥½"""
    if not preference_memory:
        return {"preferences": [], "message": "åå¥½è®°å¿†æœªåˆå§‹åŒ–"}
    
    try:
        prefs = await preference_memory.search_preferences(query, top_k)
        return {
            "preferences": [p.dict() for p in prefs],
            "count": len(prefs)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ==================== è®°å¿†ç±»å‹ç®¡ç†ç«¯ç‚¹ ====================

@app.get("/memory-types")
async def get_memory_types():
    """è·å–æ‰€æœ‰è®°å¿†ç±»å‹åŠå…¶æƒé‡"""
    return {
        "types": MEMORY_TYPE_WEIGHTS,
        "description": {
            "episodic": "æƒ…æ™¯è®°å¿† - å…·ä½“äº‹ä»¶ã€å¯¹è¯ã€ç»å†ï¼ˆæœ‰æ—¶é—´åœ°ç‚¹ï¼‰",
            "semantic": "è¯­ä¹‰è®°å¿† - æŠ½è±¡çŸ¥è¯†ã€æ¦‚å¿µã€äº‹å®ï¼ˆæ— å…·ä½“æ—¶é—´ï¼‰",
            "procedural": "ç¨‹åºè®°å¿† - æŠ€èƒ½ã€ä¹ æƒ¯ã€æ“ä½œæ–¹å¼",
            "preference": "åå¥½è®°å¿† - ç”¨æˆ·å–œå¥½ã€åŒæ¶",
            "fact": "äº‹å®è®°å¿† - å®¢è§‚äº‹å®ä¿¡æ¯",
            "tool": "å·¥å…·è®°å¿† - å·¥å…·ä½¿ç”¨è®°å½•",
            "event": "äº‹ä»¶è®°å¿† - é‡è¦äº‹ä»¶",
            "general": "é€šç”¨è®°å¿† - æœªåˆ†ç±»"
        }
    }


@app.get("/memories/by-type/{memory_type}")
async def get_memories_by_type(
    memory_type: str,
    user_id: str = Query(default=USER_ID),
    limit: int = Query(default=50, le=200)
):
    """æŒ‰ç±»å‹è·å–è®°å¿†"""
    if not qdrant_client or not qdrant_client.is_available():
        return {"memories": [], "message": "å­˜å‚¨ä¸å¯ç”¨"}
    
    valid_types = list(MEMORY_TYPE_WEIGHTS.keys())
    if memory_type not in valid_types:
        raise HTTPException(
            status_code=400, 
            detail=f"æ— æ•ˆçš„è®°å¿†ç±»å‹ã€‚æœ‰æ•ˆç±»å‹: {valid_types}"
        )
    
    try:
        # è·å–æ‰€æœ‰è®°å¿†å¹¶è¿‡æ»¤
        all_memories = qdrant_client.get_all_memories(user_id=user_id, limit=limit * 3)
        filtered = [m for m in all_memories if m.get('memory_type') == memory_type][:limit]
        
        return {
            "memory_type": memory_type,
            "memories": filtered,
            "count": len(filtered)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/memories/classify")
async def classify_memory(content: str):
    """ä½¿ç”¨ LLM å¯¹è®°å¿†å†…å®¹è¿›è¡Œç±»å‹åˆ†ç±»"""
    if not llm_config:
        raise HTTPException(status_code=503, detail="LLM æœªé…ç½®")
    
    try:
        import httpx
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹è®°å¿†å†…å®¹è¿›è¡Œåˆ†ç±»ï¼Œè¿”å›æœ€åˆé€‚çš„è®°å¿†ç±»å‹ã€‚

è®°å¿†å†…å®¹ï¼š{content}

å¯é€‰ç±»å‹ï¼š
- episodic: æƒ…æ™¯è®°å¿†ï¼ˆå…·ä½“äº‹ä»¶ã€å¯¹è¯ã€ç»å†ï¼Œæœ‰æ—¶é—´åœ°ç‚¹ï¼‰
- semantic: è¯­ä¹‰è®°å¿†ï¼ˆæŠ½è±¡çŸ¥è¯†ã€ç”¨æˆ·å±æ€§ï¼Œå¦‚"ç”¨æˆ·æ˜¯åŒ»ç”Ÿ"ï¼‰
- procedural: ç¨‹åºè®°å¿†ï¼ˆä¹ æƒ¯ã€æ“ä½œæ–¹å¼ï¼Œå¦‚"ç”¨æˆ·ä¹ æƒ¯æ™šç¡"ï¼‰
- preference: åå¥½è®°å¿†ï¼ˆå–œå¥½ã€åŒæ¶ï¼Œå¦‚"ç”¨æˆ·å–œæ¬¢ç«é”…"ï¼‰
- fact: äº‹å®è®°å¿†ï¼ˆå®¢è§‚äº‹å®ï¼‰
- event: äº‹ä»¶è®°å¿†ï¼ˆé‡è¦äº‹ä»¶ï¼‰
- general: é€šç”¨è®°å¿†ï¼ˆæ— æ³•åˆ†ç±»ï¼‰

è¯·åªè¿”å›ä¸€ä¸ªç±»å‹åç§°ï¼ˆè‹±æ–‡ï¼‰ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{llm_config['base_url']}/chat/completions",
                headers={"Authorization": f"Bearer {llm_config['api_key']}"},
                json={
                    "model": llm_config['model'],
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 50,
                    "temperature": 0.1
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                classified_type = result['choices'][0]['message']['content'].strip().lower()
                
                # éªŒè¯ç±»å‹
                if classified_type not in MEMORY_TYPE_WEIGHTS:
                    classified_type = "general"
                
                return {
                    "content": content,
                    "classified_type": classified_type,
                    "type_weight": MEMORY_TYPE_WEIGHTS.get(classified_type, 1.0)
                }
            else:
                return {"content": content, "classified_type": "general", "error": "LLM è°ƒç”¨å¤±è´¥"}
                
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ==================== å·¥å…·è®°å¿†ç«¯ç‚¹ ====================

class RecordToolUsageRequest(BaseModel):
    tool_name: str
    tool_category: str = "other"  # search, media, utility, communication, creative, system, other
    parameters: Optional[Dict[str, Any]] = None
    success: bool = True
    result_summary: Optional[str] = None
    context: Optional[str] = None
    user_intent: Optional[str] = None
    user_id: Optional[str] = USER_ID


@app.delete("/tools/{record_id}")
async def delete_tool_record(record_id: str):
    """åˆ é™¤å·¥å…·ä½¿ç”¨è®°å½•"""
    if not tool_memory:
        raise HTTPException(status_code=503, detail="å·¥å…·è®°å¿†æœªåˆå§‹åŒ–")
    
    try:
        success = await tool_memory.delete_record(record_id)
        if success:
            return {"status": "success", "message": "è®°å½•å·²åˆ é™¤"}
        else:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°è¯¥è®°å½•")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/tools/stats")
async def get_tool_stats(user_id: str = Query(default=USER_ID)):
    """è·å–å·¥å…·ä½¿ç”¨ç»Ÿè®¡"""
    if not tool_memory:
        return {"status": "disabled", "message": "å·¥å…·è®°å¿†æœªå¯ç”¨"}
    
    stats = await tool_memory.get_stats()
    return {
        "status": "enabled",
        **stats
    }


@app.post("/tools/record")
async def record_tool_usage(request: RecordToolUsageRequest):
    """è®°å½•å·¥å…·ä½¿ç”¨"""
    if not tool_memory:
        raise HTTPException(status_code=503, detail="å·¥å…·è®°å¿†æœªå¯ç”¨")
    
    try:
        from memories.tool_memory import ToolCategory
        
        # è½¬æ¢ç±»åˆ«
        try:
            category = ToolCategory(request.tool_category)
        except ValueError:
            category = ToolCategory.OTHER
        
        record = await tool_memory.record_usage(
            tool_name=request.tool_name,
            tool_category=category,
            parameters=request.parameters or {},
            success=request.success,
            result_summary=request.result_summary,
            context=request.context,
            user_intent=request.user_intent
        )
        
        return {
            "status": "success",
            "record_id": record.id,
            "tool_name": record.tool_name
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/tools/frequently-used")
async def get_frequently_used_tools(
    category: Optional[str] = None,
    top_k: int = 10,
    user_id: str = Query(default=USER_ID)
):
    """è·å–å¸¸ç”¨å·¥å…·åˆ—è¡¨"""
    if not tool_memory:
        return {"tools": [], "message": "å·¥å…·è®°å¿†æœªå¯ç”¨"}
    
    try:
        from memories.tool_memory import ToolCategory
        
        cat = ToolCategory(category) if category else None
        tools = await tool_memory.get_frequently_used_tools(cat, top_k)
        
        return {
            "tools": [
                {
                    "tool_name": t.tool_name,
                    "category": t.tool_category.value,
                    "use_count": t.use_count,
                    "success_rate": round(t.success_rate, 2),
                    "last_used": t.last_used_at.isoformat()
                }
                for t in tools
            ],
            "count": len(tools)
        }
    except Exception as e:
        return {"tools": [], "error": str(e)}


@app.get("/tools/recent")
async def get_recent_tool_usage(
    tool_name: Optional[str] = None,
    limit: int = 20,
    user_id: str = Query(default=USER_ID)
):
    """è·å–æœ€è¿‘å·¥å…·ä½¿ç”¨è®°å½•"""
    if not tool_memory:
        return {"records": [], "message": "å·¥å…·è®°å¿†æœªå¯ç”¨"}
    
    records = await tool_memory.get_recent_usage(tool_name, limit)
    
    return {
        "records": [
            {
                "id": r.id,
                "tool_name": r.tool_name,
                "category": r.tool_category.value,
                "success": r.success,
                "result_summary": r.result_summary,
                "user_intent": r.user_intent,
                "used_at": r.used_at.isoformat()
            }
            for r in records
        ],
        "count": len(records)
    }


@app.get("/tools/suggest/{tool_name}")
async def suggest_tool_parameters(tool_name: str):
    """æ ¹æ®å†å²ä½¿ç”¨å»ºè®®å·¥å…·å‚æ•°"""
    if not tool_memory:
        return {"suggestions": {}, "message": "å·¥å…·è®°å¿†æœªå¯ç”¨"}
    
    suggestions = await tool_memory.suggest_parameters(tool_name)
    return {"tool_name": tool_name, "suggestions": suggestions}


# ==================== è®°å¿†åé¦ˆä¿®æ­£ç«¯ç‚¹ ====================

class MemoryFeedbackRequest(BaseModel):
    memory_id: str
    feedback_type: str  # correct, supplement, delete, merge
    correction: Optional[str] = None  # ä¿®æ­£åçš„å†…å®¹
    reason: Optional[str] = None
    user_id: Optional[str] = USER_ID


@app.post("/memory/feedback")
async def submit_memory_feedback(request: MemoryFeedbackRequest):
    """æäº¤è®°å¿†åé¦ˆï¼ˆä¿®æ­£/è¡¥å……/åˆ é™¤/åˆå¹¶ï¼‰
    
    feedback_type:
    - correct: ä¿®æ­£è®°å¿†å†…å®¹
    - supplement: è¡¥å……ä¿¡æ¯
    - delete: æ ‡è®°åˆ é™¤
    - merge: åˆå¹¶åˆ°å…¶ä»–è®°å¿†
    """
    if not qdrant_client or not qdrant_client.is_available():
        raise HTTPException(status_code=503, detail="å­˜å‚¨ä¸å¯ç”¨")
    
    try:
        # è·å–åŸè®°å¿†
        original = qdrant_client.get_memory(request.memory_id)
        if not original:
            raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨")
        
        if request.feedback_type == "correct":
            # ä¿®æ­£è®°å¿†å†…å®¹
            if not request.correction:
                raise HTTPException(status_code=400, detail="ä¿®æ­£å†…å®¹ä¸èƒ½ä¸ºç©º")
            
            # æ›´æ–°å†…å®¹
            new_vector = encode_text(request.correction)
            payload = original.get('payload', {})
            payload['content'] = request.correction
            payload['updated_at'] = datetime.now().isoformat()
            payload['correction_history'] = payload.get('correction_history', [])
            payload['correction_history'].append({
                'original': original.get('payload', {}).get('content'),
                'corrected_at': datetime.now().isoformat(),
                'reason': request.reason
            })
            
            qdrant_client.update_memory(request.memory_id, payload, new_vector)
            
            return {
                "status": "success",
                "action": "corrected",
                "memory_id": request.memory_id,
                "new_content": request.correction
            }
        
        elif request.feedback_type == "supplement":
            # è¡¥å……ä¿¡æ¯
            if not request.correction:
                raise HTTPException(status_code=400, detail="è¡¥å……å†…å®¹ä¸èƒ½ä¸ºç©º")
            
            payload = original.get('payload', {})
            original_content = payload.get('content', '')
            supplemented_content = f"{original_content}\n[è¡¥å……] {request.correction}"
            
            new_vector = encode_text(supplemented_content)
            payload['content'] = supplemented_content
            payload['updated_at'] = datetime.now().isoformat()
            
            qdrant_client.update_memory(request.memory_id, payload, new_vector)
            
            return {
                "status": "success",
                "action": "supplemented",
                "memory_id": request.memory_id,
                "new_content": supplemented_content
            }
        
        elif request.feedback_type == "delete":
            # æ ‡è®°åˆ é™¤
            success = qdrant_client.delete_memory(request.memory_id)
            
            return {
                "status": "success" if success else "failed",
                "action": "deleted",
                "memory_id": request.memory_id
            }
        
        elif request.feedback_type == "merge":
            # åˆå¹¶åˆ°å…¶ä»–è®°å¿†ï¼ˆéœ€è¦ correction å­—æ®µæŒ‡å®šç›®æ ‡è®°å¿† IDï¼‰
            if not request.correction:
                raise HTTPException(status_code=400, detail="è¯·æŒ‡å®šç›®æ ‡è®°å¿† ID")
            
            target_id = request.correction
            target = qdrant_client.get_memory(target_id)
            if not target:
                raise HTTPException(status_code=404, detail="ç›®æ ‡è®°å¿†ä¸å­˜åœ¨")
            
            # åˆå¹¶å†…å®¹
            original_content = original.get('payload', {}).get('content', '')
            target_content = target.get('payload', {}).get('content', '')
            merged_content = f"{target_content}\n[åˆå¹¶è‡ª {request.memory_id}] {original_content}"
            
            # æ›´æ–°ç›®æ ‡è®°å¿†
            new_vector = encode_text(merged_content)
            target_payload = target.get('payload', {})
            target_payload['content'] = merged_content
            target_payload['updated_at'] = datetime.now().isoformat()
            target_payload['merge_count'] = target_payload.get('merge_count', 0) + 1
            target_payload['merged_from'] = target_payload.get('merged_from', [])
            target_payload['merged_from'].append(request.memory_id)
            
            qdrant_client.update_memory(target_id, target_payload, new_vector)
            
            # åˆ é™¤åŸè®°å¿†
            qdrant_client.delete_memory(request.memory_id)
            
            return {
                "status": "success",
                "action": "merged",
                "source_id": request.memory_id,
                "target_id": target_id,
                "merged_content": merged_content
            }
        
        else:
            raise HTTPException(status_code=400, detail=f"æœªçŸ¥çš„åé¦ˆç±»å‹: {request.feedback_type}")
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/memory/{memory_id}/history")
async def get_memory_history(memory_id: str):
    """è·å–è®°å¿†çš„ä¿®æ”¹å†å²"""
    if not qdrant_client or not qdrant_client.is_available():
        raise HTTPException(status_code=503, detail="å­˜å‚¨ä¸å¯ç”¨")
    
    memory = qdrant_client.get_memory(memory_id)
    if not memory:
        raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨")
    
    payload = memory.get('payload', {})
    
    return {
        "memory_id": memory_id,
        "current_content": payload.get('content'),
        "created_at": payload.get('created_at'),
        "updated_at": payload.get('updated_at'),
        "merge_count": payload.get('merge_count', 0),
        "merged_from": payload.get('merged_from', []),
        "correction_history": payload.get('correction_history', [])
    }


# ==================== çŸ¥è¯†åº“å¯¼å…¥ç«¯ç‚¹ ====================

class ImportDocumentRequest(BaseModel):
    source: str  # æ–‡ä»¶è·¯å¾„æˆ– URL
    tags: Optional[List[str]] = None
    extract_entities: bool = False
    user_id: Optional[str] = USER_ID


class ImportBatchRequest(BaseModel):
    sources: List[str]
    tags: Optional[List[str]] = None
    extract_entities: bool = False
    user_id: Optional[str] = USER_ID


@app.post("/kb/import")
async def import_document(request: ImportDocumentRequest):
    """å¯¼å…¥æ–‡æ¡£åˆ°çŸ¥è¯†åº“
    
    æ”¯æŒï¼š
    - æ–‡æœ¬æ–‡ä»¶ (.txt)
    - PDF æ–‡ä»¶ (.pdf)
    - Markdown æ–‡ä»¶ (.md)
    - ç½‘é¡µ URL (http/https)
    """
    if not document_loader:
        raise HTTPException(status_code=503, detail="æ–‡æ¡£åŠ è½½å™¨æœªåˆå§‹åŒ–")
    
    if not qdrant_client or not qdrant_client.is_available():
        raise HTTPException(status_code=503, detail="å­˜å‚¨ä¸å¯ç”¨")
    
    try:
        user_id = request.user_id or USER_ID
        tags = request.tags or []
        
        # åŠ è½½æ–‡æ¡£
        chunks = document_loader.load(request.source)
        
        if not chunks:
            return {
                "status": "failed",
                "message": f"æ— æ³•åŠ è½½æ–‡æ¡£: {request.source}",
                "chunks_count": 0
            }
        
        # å¯¼å…¥æ¯ä¸ªå—
        imported_count = 0
        memory_ids = []
        
        for chunk in chunks:
            content = chunk.content
            if not content or len(content) < 10:
                continue
            
            # ç”Ÿæˆå‘é‡
            vector = encode_text(content)
            
            # åˆ›å»ºè®°å¿†
            memory_id = str(uuid.uuid4())
            payload = {
                'content': content,
                'user_id': user_id,
                'importance': 0.6,
                'memory_type': 'document',
                'tags': tags + [chunk.metadata.get('type', 'document')],
                'source': chunk.source,
                'source_type': chunk.metadata.get('type'),
                'chunk_index': chunk.chunk_index,
                'created_at': datetime.now().isoformat()
            }
            
            qdrant_client.add_memory(memory_id, vector, payload)
            memory_ids.append(memory_id)
            imported_count += 1
        
        return {
            "status": "success",
            "source": request.source,
            "chunks_count": len(chunks),
            "imported_count": imported_count,
            "memory_ids": memory_ids[:10],  # åªè¿”å›å‰10ä¸ª
            "total_memory_ids": len(memory_ids)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/kb/import/batch")
async def import_documents_batch(request: ImportBatchRequest):
    """æ‰¹é‡å¯¼å…¥æ–‡æ¡£"""
    if not document_loader:
        raise HTTPException(status_code=503, detail="æ–‡æ¡£åŠ è½½å™¨æœªåˆå§‹åŒ–")
    
    results = []
    total_imported = 0
    total_failed = 0
    
    for source in request.sources:
        try:
            single_request = ImportDocumentRequest(
                source=source,
                tags=request.tags,
                extract_entities=request.extract_entities,
                user_id=request.user_id
            )
            result = await import_document(single_request)
            results.append(result)
            
            if result.get('status') == 'success':
                total_imported += result.get('imported_count', 0)
            else:
                total_failed += 1
        except Exception as e:
            results.append({
                "source": source,
                "status": "failed",
                "error": str(e)
            })
            total_failed += 1
    
    return {
        "total_sources": len(request.sources),
        "total_imported": total_imported,
        "total_failed": total_failed,
        "details": results
    }


@app.post("/kb/import/url")
async def import_from_url(
    url: str,
    tags: Optional[List[str]] = None,
    user_id: str = Query(default=USER_ID)
):
    """ä» URL å¯¼å…¥ç½‘é¡µå†…å®¹"""
    request = ImportDocumentRequest(
        source=url,
        tags=tags or ['web'],
        user_id=user_id
    )
    return await import_document(request)


# ==================== å›¾åƒè®°å¿†ç«¯ç‚¹ ====================

class UploadImageRequest(BaseModel):
    image_base64: str = Field(..., alias="image_base64", description="Base64 ç¼–ç çš„å›¾åƒ")
    filename: Optional[str] = "image.jpg"
    image_type: Optional[str] = "other"  # conversation, document, screenshot, avatar, reference, other
    description: Optional[str] = None
    tags: Optional[List[str]] = None
    user_id: Optional[str] = USER_ID
    auto_describe: bool = True
    
    class Config:
        populate_by_name = True  # å…è®¸ä½¿ç”¨å­—æ®µåæˆ–åˆ«å


@app.get("/images/stats")
async def get_image_stats(user_id: str = Query(default=USER_ID)):
    """è·å–å›¾åƒè®°å¿†ç»Ÿè®¡"""
    if not image_memory:
        return {"status": "disabled", "message": "å›¾åƒè®°å¿†æœªå¯ç”¨"}
    
    stats = image_memory.get_stats(user_id)
    return {"status": "enabled", **stats}


@app.post("/images/upload")
async def upload_image(request: UploadImageRequest):
    """ä¸Šä¼ å›¾åƒ
    
    å›¾åƒä»¥ Base64 æ ¼å¼ä¸Šä¼ ï¼Œæ”¯æŒè‡ªåŠ¨ç”Ÿæˆæè¿°
    """
    if not image_memory:
        raise HTTPException(status_code=503, detail="å›¾åƒè®°å¿†æœªå¯ç”¨")
    
    try:
        result = await image_memory.save_image_from_base64(
            base64_data=request.image_base64,
            original_name=request.filename,
            image_type=request.image_type,
            description=request.description,
            tags=request.tags,
            user_id=request.user_id,
            auto_describe=request.auto_describe
        )
        
        if result:
            return {
                "status": "success",
                "image_id": result.id,
                "filename": result.filename,
                "description": result.description,
                "size_bytes": result.size_bytes,
                "dimensions": f"{result.width}x{result.height}"
            }
        else:
            raise HTTPException(status_code=400, detail="å›¾åƒä¿å­˜å¤±è´¥")
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/images/search")
async def search_images(
    query: str,
    top_k: int = 5,
    image_type: Optional[str] = None,
    user_id: str = Query(default=USER_ID)
):
    """æœç´¢å›¾åƒ
    
    ä½¿ç”¨æ–‡æœ¬æè¿°æœç´¢ç›¸å…³å›¾åƒ
    """
    if not image_memory:
        return {"images": [], "message": "å›¾åƒè®°å¿†æœªå¯ç”¨"}
    
    results = await image_memory.search(
        query=query,
        user_id=user_id,
        top_k=top_k,
        image_type=image_type
    )
    
    return {
        "query": query,
        "images": results,
        "count": len(results)
    }


@app.get("/images/{image_id}")
async def get_image_info(image_id: str):
    """è·å–å›¾åƒä¿¡æ¯"""
    if not image_memory:
        raise HTTPException(status_code=503, detail="å›¾åƒè®°å¿†æœªå¯ç”¨")
    
    metadata = await image_memory.get_image(image_id)
    if metadata:
        return metadata.to_dict()
    raise HTTPException(status_code=404, detail="å›¾åƒä¸å­˜åœ¨")


@app.get("/images/{image_id}/data")
async def get_image_data(
    image_id: str,
    thumbnail: bool = False
):
    """è·å–å›¾åƒæ•°æ®ï¼ˆBase64ï¼‰"""
    if not image_memory:
        raise HTTPException(status_code=503, detail="å›¾åƒè®°å¿†æœªå¯ç”¨")
    
    data = await image_memory.get_image_base64(image_id, thumbnail)
    if data:
        return {
            "image_id": image_id,
            "thumbnail": thumbnail,
            "data": data
        }
    raise HTTPException(status_code=404, detail="å›¾åƒä¸å­˜åœ¨")


@app.delete("/images/{image_id}")
async def delete_image(image_id: str):
    """åˆ é™¤å›¾åƒ"""
    if not image_memory:
        raise HTTPException(status_code=503, detail="å›¾åƒè®°å¿†æœªå¯ç”¨")
    
    success = await image_memory.delete_image(image_id)
    if success:
        return {"status": "success", "message": f"å·²åˆ é™¤å›¾åƒ {image_id}"}
    raise HTTPException(status_code=404, detail="å›¾åƒä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥")


@app.get("/images")
async def list_images(
    user_id: str = Query(default=USER_ID),
    image_type: Optional[str] = None,
    limit: int = 50
):
    """åˆ—å‡ºç”¨æˆ·çš„å›¾åƒ"""
    if not image_memory:
        return {"images": [], "message": "å›¾åƒè®°å¿†æœªå¯ç”¨"}
    
    images = await image_memory.list_images(user_id, image_type, limit)
    
    return {
        "images": [m.to_dict() for m in images],
        "count": len(images)
    }


@app.post("/images/regenerate-descriptions")
async def regenerate_image_descriptions(
    user_id: str = Query(default=USER_ID),
    force: bool = Query(default=False, description="æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰æè¿°")
):
    """ä¸ºæ²¡æœ‰æè¿°çš„å›¾ç‰‡é‡æ–°ç”Ÿæˆæè¿°
    
    - force=False: åªä¸ºæ²¡æœ‰æè¿°çš„å›¾ç‰‡ç”Ÿæˆ
    - force=True: ä¸ºæ‰€æœ‰å›¾ç‰‡é‡æ–°ç”Ÿæˆæè¿°
    """
    if not image_memory:
        raise HTTPException(status_code=503, detail="å›¾åƒè®°å¿†æœªå¯ç”¨")
    
    try:
        images = await image_memory.list_images(user_id, limit=500)
        updated_count = 0
        failed_count = 0
        
        for img_meta in images:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæè¿°
            if not force and img_meta.description and img_meta.description.strip():
                continue
            
            try:
                # è¯»å–å›¾ç‰‡æ•°æ®
                img_data = await image_memory.get_image_data(img_meta.id, thumbnail=False)
                if not img_data:
                    failed_count += 1
                    continue
                
                # ç”Ÿæˆæè¿°
                from PIL import Image
                from io import BytesIO
                image = Image.open(BytesIO(img_data))
                
                description = await image_memory._generate_description(image, img_meta.original_name)
                
                if description:
                    # æ›´æ–°å…ƒæ•°æ®
                    img_meta.description = description
                    image_memory.metadata_cache[img_meta.id] = img_meta
                    updated_count += 1
                    logger.info(f"å·²ä¸ºå›¾ç‰‡ {img_meta.id} ç”Ÿæˆæè¿°: {description[:50]}...")
                else:
                    failed_count += 1
                    
            except Exception as e:
                logger.error(f"ç”Ÿæˆå›¾ç‰‡ {img_meta.id} æè¿°å¤±è´¥: {e}")
                failed_count += 1
        
        # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
        if updated_count > 0:
            image_memory._save_metadata_to_file()
        
        return {
            "status": "success",
            "message": f"å·²æ›´æ–° {updated_count} å¼ å›¾ç‰‡çš„æè¿°ï¼Œ{failed_count} å¼ å¤±è´¥",
            "updated_count": updated_count,
            "failed_count": failed_count
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ==================== å¼‚æ­¥è°ƒåº¦å™¨ç«¯ç‚¹ ====================

class SubmitTaskRequest(BaseModel):
    task_type: str  # add_memory, process_image, extract_entities
    payload: Dict[str, Any]
    priority: Optional[int] = 1  # 0=low, 1=normal, 2=high, 3=critical
    user_id: Optional[str] = USER_ID
    timeout: Optional[int] = 60


@app.get("/scheduler/stats")
async def get_scheduler_stats():
    """è·å–è°ƒåº¦å™¨ç»Ÿè®¡"""
    if not scheduler:
        return {"status": "disabled", "message": "è°ƒåº¦å™¨æœªå¯ç”¨"}
    
    stats = scheduler.get_stats()
    return {"status": "enabled", **stats}


@app.post("/scheduler/submit")
async def submit_task(request: SubmitTaskRequest):
    """æäº¤å¼‚æ­¥ä»»åŠ¡
    
    ä»»åŠ¡ç±»å‹ï¼š
    - add_memory: æ·»åŠ è®°å¿†ï¼ˆå¼‚æ­¥å¤„ç†ï¼‰
    - process_image: å¤„ç†å›¾åƒ
    - extract_entities: æå–å®ä½“
    """
    if not scheduler:
        raise HTTPException(status_code=503, detail="è°ƒåº¦å™¨æœªå¯ç”¨")
    
    try:
        from core.scheduler import TaskPriority
        
        # è½¬æ¢ä¼˜å…ˆçº§
        priority_map = {
            0: TaskPriority.LOW,
            1: TaskPriority.NORMAL,
            2: TaskPriority.HIGH,
            3: TaskPriority.CRITICAL
        }
        priority = priority_map.get(request.priority, TaskPriority.NORMAL)
        
        task_id = await scheduler.submit(
            task_type=request.task_type,
            payload=request.payload,
            priority=priority,
            user_id=request.user_id,
            timeout=request.timeout
        )
        
        return {
            "status": "submitted",
            "task_id": task_id,
            "task_type": request.task_type,
            "priority": request.priority
        }
    
    except RuntimeError as e:
        raise HTTPException(status_code=429, detail=str(e))  # é…é¢è¶…é™
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/scheduler/task/{task_id}")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if not scheduler:
        raise HTTPException(status_code=503, detail="è°ƒåº¦å™¨æœªå¯ç”¨")
    
    status = await scheduler.get_task_status(task_id)
    if status:
        return status
    raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")


@app.post("/async/add")
async def async_add_memory(request: AddRawMemoryRequest):
    """å¼‚æ­¥æ·»åŠ è®°å¿†
    
    å°†è®°å¿†æ·»åŠ ä»»åŠ¡æäº¤åˆ°é˜Ÿåˆ—å¼‚æ­¥å¤„ç†
    åŒæ­¥æ¥å£: POST /add_raw
    """
    if not scheduler:
        # å›é€€åˆ°åŒæ­¥å¤„ç†
        return await add_memory_raw(request)
    
    task_ids = []
    for msg in request.messages:
        task_id = await scheduler.submit(
            task_type='add_memory',
            payload={
                'content': msg.content,
                'importance': msg.importance,
                'memory_type': msg.memory_type,
                'user_id': request.user_id
            },
            user_id=request.user_id
        )
        task_ids.append(task_id)
    
    return {
        "status": "submitted",
        "message": f"å·²æäº¤ {len(task_ids)} ä¸ªä»»åŠ¡",
        "task_ids": task_ids
    }


# ==================== å›¾è°±ç«¯ç‚¹ ====================

@app.get("/graph/stats")
async def get_graph_stats():
    """è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡"""
    if not neo4j_client or not neo4j_client.is_available():
        return {"status": "disabled", "message": "çŸ¥è¯†å›¾è°±æœªå¯ç”¨"}
    
    stats = neo4j_client.get_stats()
    return {
        "status": "enabled",
        "entity_count": stats.get('entity_count', 0),
        "relation_count": stats.get('relation_count', 0)
    }


@app.get("/graph/entities")
async def list_entities(
    user_id: str = Query(default=USER_ID),
    entity_type: Optional[str] = None,
    limit: int = 50
):
    """åˆ—å‡ºå®ä½“"""
    if not neo4j_client or not neo4j_client.is_available():
        return {"entities": [], "message": "çŸ¥è¯†å›¾è°±æœªå¯ç”¨"}
    
    entities = neo4j_client.list_entities(user_id, entity_type, limit)
    return {"entities": entities, "count": len(entities)}


@app.get("/graph/relations")
async def list_relations(
    user_id: str = Query(default=USER_ID),
    limit: int = 500
):
    """åˆ—å‡ºæ‰€æœ‰å…³ç³»ï¼ˆç”¨äºå›¾è°±å¯è§†åŒ–ï¼‰"""
    if not neo4j_client or not neo4j_client.is_available():
        return {"relations": [], "message": "çŸ¥è¯†å›¾è°±æœªå¯ç”¨"}
    
    try:
        # è·å–æ‰€æœ‰å…³ç³»
        relations = neo4j_client.list_all_relations(user_id, limit)
        return {"relations": relations, "count": len(relations)}
    except Exception as e:
        logger.warning(f"è·å–å…³ç³»åˆ—è¡¨å¤±è´¥: {e}")
        return {"relations": [], "error": str(e)}


# ==================== å®ä½“æå–ç«¯ç‚¹ ====================

class ExtractEntitiesRequest(BaseModel):
    text: str
    context: Optional[str] = None
    store_to_graph: bool = False  # æ˜¯å¦å­˜å‚¨åˆ°å›¾è°±
    link_to_memory_id: Optional[str] = None  # å…³è”çš„è®°å¿† ID
    user_id: Optional[str] = USER_ID


@app.post("/entities/extract")
async def extract_entities(request: ExtractEntitiesRequest):
    """ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»
    
    ä½¿ç”¨ LLM è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬ä¸­çš„å®ä½“å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚
    å¯é€‰æ‹©æ˜¯å¦å­˜å‚¨åˆ°çŸ¥è¯†å›¾è°±ã€‚
    """
    if not entity_extractor:
        raise HTTPException(status_code=503, detail="å®ä½“æå–å™¨æœªå¯ç”¨ï¼ˆéœ€è¦é…ç½® entity_extraction.enabled=trueï¼‰")
    
    try:
        # æå–å®ä½“å’Œå…³ç³»
        entities, relations = await entity_extractor.extract(
            request.text,
            request.context
        )
        
        result = {
            "entities": [
                {
                    "name": e.name,
                    "type": e.entity_type.value,
                    "description": e.description,
                    "confidence": e.confidence
                }
                for e in entities
            ],
            "relations": [
                {
                    "source": r.source_name,
                    "target": r.target_name,
                    "type": r.relation_type.value,
                    "description": r.description,
                    "confidence": r.confidence
                }
                for r in relations
            ],
            "entity_count": len(entities),
            "relation_count": len(relations)
        }
        
        # å­˜å‚¨åˆ°å›¾è°±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if request.store_to_graph and neo4j_client and neo4j_client.is_available():
            stored_entities = []
            entity_name_to_id = {}
            
            for entity in entities:
                ent_id = f"ent_{uuid.uuid4().hex[:12]}"
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = neo4j_client.find_entity_by_name(entity.name, request.user_id)
                if existing:
                    ent_id = existing['id']
                    if request.link_to_memory_id and hasattr(neo4j_client, 'link_entity_to_memory'):
                        neo4j_client.link_entity_to_memory(ent_id, request.link_to_memory_id)
                else:
                    props = {
                        'description': entity.description,
                        'confidence': entity.confidence
                    }
                    if request.link_to_memory_id:
                        props['source_memory_ids'] = [request.link_to_memory_id]
                    
                    neo4j_client.add_entity(
                        entity_id=ent_id,
                        entity_type=entity.entity_type.value,
                        name=entity.name,
                        properties=props,
                        user_id=request.user_id
                    )
                
                entity_name_to_id[entity.name] = ent_id
                stored_entities.append({"id": ent_id, "name": entity.name})
            
            # å­˜å‚¨å…³ç³»
            stored_relations = 0
            for relation in relations:
                src_id = entity_name_to_id.get(relation.source_name)
                tgt_id = entity_name_to_id.get(relation.target_name)
                if src_id and tgt_id:
                    neo4j_client.add_relation(
                        source_id=src_id,
                        target_id=tgt_id,
                        relation_type=relation.relation_type.value,
                        properties={
                            'description': relation.description,
                            'confidence': relation.confidence,
                            'source_memory_id': request.link_to_memory_id
                        }
                    )
                    stored_relations += 1
            
            result["stored_to_graph"] = True
            result["stored_entities"] = stored_entities
            result["stored_relations"] = stored_relations
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/entities/stats")
async def get_entity_stats(user_id: str = Query(default=USER_ID)):
    """è·å–å®ä½“ç»Ÿè®¡"""
    if not entity_extractor:
        return {"status": "disabled", "message": "å®ä½“æå–å™¨æœªå¯ç”¨"}
    
    stats = {"status": "enabled"}
    
    if neo4j_client and neo4j_client.is_available():
        graph_stats = neo4j_client.get_stats(user_id)
        stats.update({
            "entity_count": graph_stats.get('entity_count', 0),
            "relation_count": graph_stats.get('relation_count', 0),
            "entity_types": graph_stats.get('entity_types', {}),
            "relation_types": graph_stats.get('relation_types', {})
        })
    
    return stats


@app.post("/graph/query/related")
async def find_related(
    entity_id: str,
    max_depth: int = 2
):
    """æŸ¥æ‰¾ç›¸å…³å®ä½“"""
    if not neo4j_client or not neo4j_client.is_available():
        return {"related": [], "message": "çŸ¥è¯†å›¾è°±æœªå¯ç”¨"}
    
    related = neo4j_client.find_related_entities(entity_id, max_depth)
    return {"entity_id": entity_id, "related": related}


class AddEntityRequest(BaseModel):
    entity_id: Optional[str] = None
    entity_type: str
    name: str
    properties: Optional[Dict[str, Any]] = None
    user_id: Optional[str] = USER_ID


class AddRelationRequest(BaseModel):
    source_id: str
    target_id: str
    relation_type: str
    properties: Optional[Dict[str, Any]] = None


@app.post("/graph/entity")
async def add_entity(request: AddEntityRequest):
    """æ·»åŠ å®ä½“"""
    if not neo4j_client or not neo4j_client.is_available():
        raise HTTPException(status_code=503, detail="å›¾æ•°æ®åº“æœªå¯ç”¨")
    
    entity_id = request.entity_id or str(uuid.uuid4())
    success = neo4j_client.add_entity(
        entity_id=entity_id,
        entity_type=request.entity_type,
        name=request.name,
        properties=request.properties,
        user_id=request.user_id
    )
    
    if success:
        return {"status": "success", "entity_id": entity_id, "name": request.name}
    else:
        raise HTTPException(status_code=500, detail="æ·»åŠ å®ä½“å¤±è´¥")


@app.get("/graph/entity/{entity_id}")
async def get_entity(entity_id: str):
    """è·å–å®ä½“è¯¦æƒ…"""
    if not neo4j_client or not neo4j_client.is_available():
        raise HTTPException(status_code=503, detail="å›¾æ•°æ®åº“æœªå¯ç”¨")
    
    entity = neo4j_client.get_entity(entity_id)
    if entity:
        return entity
    else:
        raise HTTPException(status_code=404, detail="å®ä½“ä¸å­˜åœ¨")


@app.delete("/graph/entity/{entity_id}")
async def delete_entity(entity_id: str):
    """åˆ é™¤å®ä½“"""
    if not neo4j_client or not neo4j_client.is_available():
        raise HTTPException(status_code=503, detail="å›¾æ•°æ®åº“æœªå¯ç”¨")
    
    success = neo4j_client.delete_entity(entity_id)
    if success:
        return {"status": "success", "message": f"å·²åˆ é™¤å®ä½“ {entity_id}"}
    else:
        raise HTTPException(status_code=404, detail="å®ä½“ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥")


@app.post("/graph/relation")
async def add_relation(request: AddRelationRequest):
    """æ·»åŠ å…³ç³»"""
    if not neo4j_client or not neo4j_client.is_available():
        raise HTTPException(status_code=503, detail="å›¾æ•°æ®åº“æœªå¯ç”¨")
    
    success = neo4j_client.add_relation(
        source_id=request.source_id,
        target_id=request.target_id,
        relation_type=request.relation_type,
        properties=request.properties
    )
    
    if success:
        return {
            "status": "success",
            "relation": f"{request.source_id} -[{request.relation_type}]-> {request.target_id}"
        }
    else:
        raise HTTPException(status_code=400, detail="æ·»åŠ å…³ç³»å¤±è´¥ï¼ˆè¯·ç¡®ä¿ä¸¤ä¸ªå®ä½“éƒ½å­˜åœ¨ï¼‰")


@app.get("/graph/entity/{entity_id}/relations")
async def get_entity_relations(
    entity_id: str,
    direction: str = Query(default="both", regex="^(in|out|both)$")
):
    """è·å–å®ä½“çš„æ‰€æœ‰å…³ç³»"""
    if not neo4j_client or not neo4j_client.is_available():
        raise HTTPException(status_code=503, detail="å›¾æ•°æ®åº“æœªå¯ç”¨")
    
    relations = neo4j_client.get_relations(entity_id, direction)
    return {"entity_id": entity_id, "relations": relations, "count": len(relations)}


@app.post("/graph/search")
async def graph_search(entity_names: List[str], user_id: str = USER_ID):
    """æ ¹æ®å®ä½“åç§°æœç´¢å›¾è°±"""
    if not neo4j_client or not neo4j_client.is_available():
        return {"results": [], "message": "å›¾æ•°æ®åº“æœªå¯ç”¨"}
    
    results = neo4j_client.search_by_entities(entity_names, user_id)
    return {"results": results, "count": len(results)}


@app.get("/graph/path")
async def find_path(source_id: str, target_id: str, max_length: int = 5):
    """æŸ¥æ‰¾ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„è·¯å¾„"""
    if not neo4j_client or not neo4j_client.is_available():
        raise HTTPException(status_code=503, detail="å›¾æ•°æ®åº“æœªå¯ç”¨")
    
    path = neo4j_client.find_path(source_id, target_id, max_length)
    if path:
        return {"path": path, "length": len(path)}
    else:
        return {"path": None, "message": "æœªæ‰¾åˆ°è·¯å¾„"}


# ==================== ä¸»å…¥å£ ====================

if __name__ == "__main__":
    print("=" * 60)
    print("  MemOS è®°å¿†æœåŠ¡ v2.0 (å®Œæ•´é›†æˆç‰ˆ)")
    print("=" * 60)
    print("  ç«¯å£: 8003")
    print("  æ–‡æ¡£: http://127.0.0.1:8003/docs")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8003)
